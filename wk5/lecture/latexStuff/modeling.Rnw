%%% Title:    Statistical Modeling
%%% Author:   Kyle M. Lang
%%% Created:  2018-04-13
%%% Modified: 2021-11-24

\documentclass[10pt]{beamer}
\usetheme{Utrecht}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}

\title{Statistical Modeling}
\subtitle{Fundamental Techniques in Data Science}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Utrecht University}
\date{}


\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
set.seed(235711)

library(knitr)
library(ggplot2)
library(MASS)
library(DAAG)
library(xtable)
library(MLmetrics)

source("../../../code/supportFunctions.R")

options(width = 60)
opts_chunk$set(size = "footnotesize",
               fig.align = "center",
               fig.path = "figure/modeling-",
               message = FALSE,
               warning = FALSE,
               comment = "")
knit_theme$set('edit-kwrite')
@

%------------------------------------------------------------------------------%

\begin{frame}[t,plain]
  \titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%------------------------------------------------------------------------------%

\section{Statistical Modeling}

%------------------------------------------------------------------------------%


\begin{frame}[allowframebreaks]{Statistical Reasoning}

  Statistics and data science are used to answer questions about hypothetical
  populations.
  \begin{itemize}
  \item Do men have higher job satisfaction than women?
  \item Can I predict your voting behavior?
  \item Can I detect groups of people who share similar attitudes towards
    climate change?
  \end{itemize}
  \vb
  To answer these questions, we need to use \emph{statistical reasoning}.
  \begin{itemize}
  \item The foundation of all good statistical analyses is a deliberate,
    careful, and thorough consideration of uncertainty.
  \end{itemize}

  \pagebreak

  If I measure a mean satisfaction rating for men of 5.6 and a mean satisfaction
  rating for women of 5.1, does that imply higher job satisfaction for men?
  \begin{itemize}
  \item Maybe...
  \item If the satisfaction ratings are highly variable, with respect to the
    size of the mean difference, we may not care much about the observed mean
    difference.
  \item The \emph{observed} mean difference may not represent a \emph{true} mean
    difference in the population.
  \end{itemize}
  \vb
  The purpose of statistics is to systematize the way that we account for
  uncertainty when making data-based decisions.


\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Statistical Modeling}

  To implement this ``statistical reasoning,'' we could use two different
  approaches: \emph{statistical testing} or \emph{statistical modeling}.
  \begin{itemize}
  \item In experimental contexts, real-world ``messiness'' is controlled through
    random assignment, and statistical testing is a sufficient method of
    knowledge generation.
    \vc
  \item Apart from A/B testing, data scientists rarely have the luxury of being
    able to conduct experiments.
    \vc
  \item Data scientists work with messy observational data and often don't have
    questions that lend themselves to straight-forward testing.
  \end{itemize}
  \vb
  Data scientists need \emph{statistical modeling}.

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Statistical Modeling}

  Modelers attempt to build a mathematical representation of the (interesting
  aspects) of a data distribution.
  \begin{itemize}
  \item The model succinctly describes whatever system is being analyzed.
    \vc
  \item Beginning with a model ensures that we are learning the important
    features of a distribution.
    \vc
  \item The modeling approach is especially important in messy data science
    applications.
  \end{itemize}

\end{frame}

\sectionslide{Different Flavors of Statistical Modeling}

%------------------------------------------------------------------------------%

\begin{frame}{Two Modeling Traditions}

  \citet{breiman:2001} defines two cultures of statistical modeling:
  \begin{itemize}
  \item Data models
  \item Algorithmic models
  \end{itemize}
  \vb
  \pause
  Data scientists use both types of models.
  \begin{itemize}
  \item Both types of model have strengths and weaknesses.
    \begin{itemize}
    \item Data models tend to support a priori hypothesis testing more easily.
    \item Data models also tend to provide more interpretable results.
    \item Algorithmic models can't be beat for pure power.
    \end{itemize}
    \vb
    \pause
  \item Algorithmic models are currently preferred in cutting edge
    prediction/classification applications.
    \vb
    \pause
  \item Many models can be viewed as data models or algorithmic models,
    depending on how they're used.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Characteristics of Models}

  Data models share several core features:
  \begin{itemize}
  \item Data models are built from probability distributions.
    \begin{itemize}
    \item Data models are modular.
    \end{itemize}
    \vc
  \item Data models encode our hypothesized understanding of the system we're
    exploring.
    \begin{itemize}
    \item Data models are constructed in a ``top-down'', theory-driven way.
    \end{itemize}
  \end{itemize}

  \pause
  \vb

  Algorithmic models are distinct from data models in several ways:
  \begin{itemize}
  \item Algorithmic models do not have to be built from probability
    distributions.
    \begin{itemize}
    \item Often, they are based on a set of decision rules (i.e., an algorithm).
    \end{itemize}
    \vc
  \item Algorithmic models begin with an objective (i.e., a problem to solve)
    and seek the optimal solution, given the data.
    \begin{itemize}
    \item They are built in a ``bottom-up'', data-driven way.
    \end{itemize}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\subsection{Data Modeling}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Data Modeling Example}

  Suppose we believe the following:
  \begin{enumerate}
  \item BMI is positively associated with disease progression in diabetic
    patients after controlling for age and average blood pressure.
  \item After controlling for age and average blood pressure, the effect of BMI
    on disease progression is different for men and women.
  \end{enumerate}
  \vb
  We can represent these beliefs with a moderated regression model:
  \begin{align*}
    Y_{prog} = \beta_0 + \beta_1 X_{BMI} + \beta_2 X_{sex} + \beta_3 X_{age} + \beta_4 X_{BP} + \beta_5 X_{BMI} X_{sex} + \varepsilon
  \end{align*}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Data Modeling Example}

  We can use R to fit our model to some patient data:

<<messages = FALSE>>=
library(dplyr)
library(rockchalk)

## Load the data:
diabetes <- readRDS("../data/diabetes.rds")
diabetes <- rename(diabetes, sex = sexF)

## Fit the regression model:
fit <- lm(progress ~ bmi * sex + age + bp, data = diabetes)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Data Modeling Example}

<<>>=
partSummary(fit, -c(1, 2))
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Data Modeling Example}

  We can do a simple slopes analysis to test the group-specific effects of BMI
  on disease progression:

<<include = FALSE>>=
psOut <- plotSlopes(fit, plotx = "bmi", modx = "sex")
tsOut <- testSlopes(psOut)
@

<<eval = FALSE>>=
psOut <- plotSlopes(fit, plotx = "bmi", modx = "sex")
tsOut <- testSlopes(psOut)
@

<<>>=
tsOut$hypotests[ , -1]
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[fragile]{Data Modeling Example}

  We can also visualize the simple slopes:

<<echo = FALSE, out.width = '6.5cm'>>=
plotSlopes(fit, plotx = "bmi", modx = "sex")
@

\end{frame}

%------------------------------------------------------------------------------%

\subsection{Algorithmic Modeling}

%------------------------------------------------------------------------------%

\begin{frame}{Algorithmic Modeling Example}

  Suppose we want to find the best predictors of disease progression among the
  variables contained in our dataset:
  \begin{columns}
    \begin{column}{0.4\textwidth}
      \begin{itemize}
      \item Age
      \item BMI
      \item Blood Pressure
      \item Blood Glucose
      \item Sex
      \end{itemize}

    \end{column}
    \begin{column}{0.4\textwidth}

      \begin{itemize}
      \item Total Cholesterol
      \item LDL Cholesterol
      \item HDL Cholesterol
      \item Triglycerides
      \item Lamorigine
      \end{itemize}

    \end{column}
  \end{columns}
  \va
  We could try \emph{best-subset selection}.
  \begin{itemize}
  \item Fit a series of regression models wherein disease progression is
    predicted by all possible subsets of X variables.
  \item Choose the set of X variables that minimizes the prediction error.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Algorithmic Modeling Example}

<<>>=
library(leaps)

## Save the predictor variables' names:
xNames <- grep(pattern = "progress",
               x       = colnames(diabetes),
               invert  = TRUE,
               value   = TRUE)

## Train the models:
fit <- regsubsets(x     = progress ~ .,
                  data  = diabetes,
                  nvmax = ncol(diabetes) - 1)

## Summarize the results:
sum <- summary(fit)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Algorithmic Modeling Example}

<<>>=
sum$outmat
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Algorithmic Modeling Example}

<<>>=
## Variables selected by BIC:
xNames[with(sum, which[which.min(bic), -1])]

## Variables selected by Adjusted R^2:
xNames[with(sum, which[which.max(adjr2), -1])]

## Variables selected by Mallow's Cp:
xNames[with(sum, which[which.min(cp), -1])]
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Algorithmic Modeling Example}

  The results seem to be highly sensitive to the error measure. What should we
  do?
  \pause
  \begin{itemize}
  \item We could pick our favorite error measure and use its results.
  \item We could throw our hands up in defeat and quit.
  \item We could look at the results and pick the answer we like best.
    \begin{itemize}
    \item The previous two suggestions are sub-optimal, but this one is
      actually unethical. Don't do this!
    \end{itemize}
  \end{itemize}
  \pause
  \vb
  If we think like a data scientist and get creative, we don't need to settle
  for these ambiguous results.
  \begin{itemize}
  \item We could implement a more robust method of calculating prediction error
    like \emph{K-fold cross validation}.
  \item We can use resampling methods to quantify uncertainty in the variable
    selection process.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Algorithmic Modeling Example}

<<>>=
bic <- r2 <- cp <- matrix(NA, 100, ncol(diabetes) - 1)
for(rp in 1 : 100) {
    ## Resample the data:
    tmp <- diabetes[sample(1 : nrow(diabetes), nrow(diabetes), TRUE), ]

    ## Train the models:
    fit <- regsubsets(x     = progress ~ .,
                      data  = tmp,
                      nvmax = ncol(tmp) - 1)
    sum <- summary(fit)

    ## Save the optimal selections:
    bic[rp, ] <- with(sum, which[which.min(bic), -1])
    r2[rp, ]  <- with(sum, which[which.max(adjr2), -1])
    cp[rp, ]  <- with(sum, which[which.min(cp), -1])
}
@

<<echo = FALSE>>=
colnames(bic) <- colnames(r2) <- colnames(cp) <- colnames(sum$which)[-1]
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Algorithmic Modeling Example}

<<size = 'scriptsize'>>=
colMeans(bic)
colMeans(r2)
colMeans(cp)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Algorithmic Modeling Example}

<<>>=
## Find the best subset via majority vote:
votes <- colMeans(rbind(bic, r2, cp)); round(votes, 3)

preds <- xNames[votes > 0.5]; preds

## Fit the winning model to the original data:
form <- paste0("progress ~ ",
               paste(preds, collapse = " + ")
               )
fit  <- lm(form, data = diabetes)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Algorithmic Modeling Example}

<<>>=
partSummary(fit, -c(1, 2))
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{References}

  \bibliographystyle{apacite}
  \bibliography{../../../literature/bibtex_files/refs.bib}

\end{frame}

%------------------------------------------------------------------------------%

\end{document}
