%%% Title:    Regression Assumptions & Diagnostics
%%% Author:   Kyle M. Lang
%%% Created:  2018-04-12
%%% Modified: 2023-12-09

\documentclass[10pt]{beamer}
\usetheme{Utrecht}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
%\usepackage{fancybox}
\usepackage{booktabs}
\usepackage{relsize}
\usepackage{listings}
\usepackage{xspace}
\usepackage{tcolorbox}
\tcbuselibrary{listings}
\usepackage{hyperref}

\newcommand{\eqit}[1]{\textrm{\textit{#1}}}

\definecolor{codebackground}{RGB}{224,234,238}
\definecolor{codestring}{RGB}{191,3,3}
\definecolor{codekeyword}{RGB}{1,1,129}
\definecolor{codecomment}{RGB}{131,129,131}

\newcommand{\src}[1]{%
  \tcbox[%
  on line,
  colback = codebackground,
  colframe = codebackground,
  left = 0pt,
  right = 0pt,
  top = 0pt,
  bottom = 0pt%
  ]{%
    \lstinline[%
    language = R,
    basicstyle = \ttfamily,
    keywordstyle = \color{codekeyword},
    commentstyle = \color{codecomment}\itshape,
    stringstyle = \color{codestring},
    deletekeywords = {_}
    % frame = single,
    % frameround = tttt,
    % fillcolor = \color{blue}%
    ]{#1}%
  }
}

\newcommand{\rmsc}[1]{\textrm{\textsc{#1}}}
\newcommand{\pkg}[1]{\textbf{#1}}
\newcommand{\pipe}{\texttt{\%>\%}}
\newcommand{\expipe}{\texttt{\%\$\%}}
\newcommand{\apipe}{\texttt{\%<>\%}}
\newcommand{\rpipe}{\texttt{|>}}

\title{Regression Assumptions \& Diagnostics}
\subtitle{Fundamental Techniques in Data Science with R}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Utrecht University}
\date{}

\begin{document}

<<setup, include = FALSE, cache = FALSE>>=
set.seed(235711)

dataDir <- "../../../data/"
codeDir <- "../../../code/"

library(knitr)
library(ggplot2)
library(MASS)
library(DAAG)
library(xtable)
library(MLmetrics)
library(dplyr)

source(paste0(codeDir, "supportFunctions.R"))

options(width = 60)
opts_chunk$set(size = "footnotesize",
               fig.align = "center",
               fig.path = "figure/assumptions-",
               message = FALSE,
               comment = "")
knit_theme$set('edit-kwrite')
@

%------------------------------------------------------------------------------%

\begin{frame}[t,plain]
  \titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  \tableofcontents
\end{frame}
%------------------------------------------------------------------------------%

\sectionslide{Assumptions \& Diagnostics}

%------------------------------------------------------------------------------%

\begin{frame}{Why Assume?}

  Consider the following equation:
  \begin{align*}
    5 = x + y
  \end{align*}
  What are the values of $x$ and $y$?
  \pause
  \begin{align*}
    y = 5 - x
  \end{align*}
  \pause
  What if we assume that $y = x$?
  \pause
  \begin{align*}
    5 &= x + y\\
    0 &= x - y
  \end{align*}
  \pause
  Now we have enough information:
  \begin{align*}
    5 = x + x = 2x~~\Rightarrow~~x = y = 2.5
  \end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Assumptions of MLR}

  The assumptions of the linear model can be stated as follows:
  \vb
  \begin{enumerate}
  \item The model is linear in the parameters.
    \vc
    \begin{itemize}
    \item This is OK: $Y = \beta_0 + \beta_1X + \beta_2Z + \beta_3XZ + \beta_4X^2 + \beta_5X^3 + \varepsilon$
      \vc
    \item This is not: $Y = \beta_0 X^{\beta_1} + \varepsilon$
    \end{itemize}
    \vb
  \item The predictor matrix is \emph{full rank}.
    \vc
    \begin{itemize}
    \item $N > P$
      \vc
    \item No $X_p$ can be a linear combination of other predictors.
    \end{itemize}

    \pagebreak

  \item The predictors are strictly exogenous.\label{exo}
    \vc
    \begin{itemize}
    \item The predictors do not correlated with the errors.
      \vc
    \item $\textrm{Cov}(\hat{Y}, \varepsilon) = 0$
      \vc
    \item $\textrm{E}[\varepsilon_n] = 0$
    \end{itemize}
    \vb
  \item The errors have constant, finite variance.\label{constVar}
    \vc
    \begin{itemize}
    \item $\textrm{Var}(\varepsilon_n) = \sigma^2 < \infty$
    \end{itemize}
    \vb
  \item The errors are uncorrelated.\label{indErr}
    \vc
    \begin{itemize}
    \item $\textrm{Cov}(\varepsilon_i, \varepsilon_j) = 0, ~ i \neq j$
    \end{itemize}
    \vb
  \item The errors are normally distributed.\label{normErr}
    \vc
    \begin{itemize}
    \item $\varepsilon \sim \textrm{N}(0, \sigma^2)$
    \end{itemize}
  \end{enumerate}

  \pagebreak

  The assumption of \emph{spherical errors} combines Assumptions \ref{constVar}
  and \ref{indErr}.
  \begin{align*}
    \textrm{Var}(\varepsilon) =
    \begin{bmatrix}
      \sigma^2 & 0 & \cdots & 0\\
      0 & \sigma^2 & \cdots & 0\\
      0 & 0 & \ddots & 0\\
      0 & 0 & \cdots & \sigma^2
    \end{bmatrix} =
    \sigma^2\mathbf{I}_N
  \end{align*}
  We can combine Assumptions \ref{exo}, \ref{constVar}, \ref{indErr}, and
  \ref{normErr} by assuming independent and identically distributed normal
  errors:
  \begin{itemize}
  \item $\varepsilon \overset{iid}{\sim} \textrm{N}(0, \sigma^2)$
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Consequences of Violating Assumptions}

  \begin{enumerate}
  \item If the model is not linear in the parameters, then we're not even
    working with linear regression.
    \begin{itemize}
    \item We need to move to entirely different modeling paradigm.
    \end{itemize}
    \vb
  \item If the predictor matrix is not full rank, the model is not estimable.
    \begin{itemize}
    \item The parameter estimates cannot be uniquely determined from the data.
    \end{itemize}
    \vb
  \item If the predictors are not exogenous, the estimated regression
    coefficients will be biased.
    \vb
  \item If the errors are not spherical, the standard errors will be biased.
    \begin{itemize}
    \item The estimated regression coefficients will be unbiased, though.
    \end{itemize}
    \vb
  \item If errors are non-normal, small-sample inferences may be biased.
    \begin{itemize}
    \item The justification for some tests and procedures used in regression
      analysis may not hold.
    \end{itemize}
  \end{enumerate}

\end{frame}

%------------------------------------------------------------------------------%

\subsection{Regression Diagnostics}

%------------------------------------------------------------------------------%

\begin{frame}{Regression Diagnostics}

  If some of the assumptions are (grossly) violated, the inferences we make
  using the model may be wrong.
  \begin{itemize}
  \item We need to check the tenability of our assumptions before leaning too
    heavily on the model estimates.
  \end{itemize}
  \vb
  These checks are called \emph{regression diagnostics}.
  \begin{itemize}
  \item Graphical visualizations
    \vc
  \item Quantitative indices/measures
    \vc
  \item Formal statistical tests
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Residual Plots}

  One of the most useful diagnostic graphics is the plot of residuals vs.
  predicted values.

  \vb

  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, message = FALSE>>=
data(anscombe)
out1 <- lm(y1 ~ x1, data = anscombe)

p0 <- with(anscombe, gg0(x = x1, y = y1))
p0 + geom_smooth(method = "lm", se = FALSE) +
    xlab("X") +
    ylab("Y")
@

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p0 <- gg0(x = predict(out1), y = resid(out1))
p0 + geom_hline(yintercept = 0, colour = "gray") +
    xlab("Predicted Y") +
    ylab("Residual Y")
@

    \end{column}
  \end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Residual Plots}

  We can easily generate a simple plot of residuals vs. fitted values by
  plotting the fitted \texttt{lm()} object in R.

  \begin{columns}
    \begin{column}{0.5\textwidth}

      <<eval = FALSE>>=
      out1 <- lm(Price ~ Horsepower,
                 data = Cars93)

      plot(out1, 1)
      @

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
out1 <- lm(Price ~ Horsepower, data = Cars93)
plot(out1, 1)
@

    \end{column}
  \end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Heteroscedasticity}

  Non-constant error variance (\emph{heteroscedasticity}) violates
  Assumption \ref{constVar}.

  \begin{columns}
    \begin{column}{0.5\textwidth}

<<>>=
plot(out1, 1)
@

    \end{column}
    \begin{column}{0.5\textwidth}

<<>>=
plot(out1, 3)
@

    \end{column}
  \end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Consequences of Heteroscedasticity}

  Non-constant error variance will not bias the parameter estimates.
  \begin{itemize}
  \item The best fit line is still correct.
  \item Our measure of uncertainty around that best fit line is wrong.
  \end{itemize}
  \vb
  Heteroscedasticity will bias standard errors (usually downward).
  \begin{itemize}
  \item Test statistics will be too large.
  \item CIs will be too narrow.
  \item We will have inflated Type I error rates.
  \end{itemize}
  \vb
  To get valid inference, we need to address (severe) heteroscedasticity.

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Treating Heteroscedasticity}

  \begin{enumerate}
  \item Transform your outcome using a concave function (e.g., $\ln(Y)$,
    $\sqrt{Y}$).
    \vc
    \begin{itemize}
    \item These transformations will shrink extreme values more than
      small/moderate ones.
      \vc
    \item It's usually a good idea to first shift the variable's scale by
      setting the minimum value to 1.
    \end{itemize}
    \vb
  \item Refit the model using \emph{weighted least squares}.
    \vc
    \begin{itemize}
    \item Create inverse weights using functions of the residual variances or
      quantities highly correlated therewith.
    \end{itemize}
    \vb
  \item Use a \emph{Heteroscedasticity Consistent} (HC) estimate of the
    asymptotic covariance matrix.
    \vc
    \begin{itemize}
    \item Robust standard errors, Huber-White standard errors, Sandwich
      estimators
    \item HC estimators correct the standard errors for non-constant error
      variance.
    \end{itemize}
  \end{enumerate}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

<<>>=
## The 'sandwich' package provides several HC estimators:
library(sandwich)

## the 'lmtest' package provides fancy testing tools for linear models:
library(lmtest)

## Use sandwich estimator to compute ACOV matrix:
hcCov <- vcovHC(out1)

## Test coefficients with robust SEs:
robTest <- coeftest(out1, vcov = hcCov)

## Test coefficients with default SEs:
defTest <- summary(out1)$coefficients
@

\pagebreak

<<>>=
## Compare robust and default approaches:
robTest
defTest
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Correlated Errors}

  Errors can become correlated in two basic ways:
  \vb
  \begin{enumerate}
  \item Serial dependence
    \begin{itemize}
    \item When modeling longitudinal data, the errors for a given observational
      unit are correlated over time.
      \vc
    \item We can detect temporal dependence by examining the
      \emph{autocorrelation} of the residuals.
    \end{itemize}
    \vb
  \item Clustering
    \begin{itemize}
    \item Your data have some important, unmodeled, grouping structure.
      \begin{itemize}
      \item Children nested within classrooms
      \item Romantic couples
      \item Departments within a company
      \end{itemize}
      \vc
    \item We can detect problematic levels of clustering with the
      \emph{intraclass correlation coefficient} (ICC).
      \begin{itemize}
      \item We need to know the clustering variable to apply the ICC.
      \end{itemize}
    \end{itemize}
  \end{enumerate}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Treating Correlated Errors}

  Serially dependent errors in a longitudinal model usually indicate an
  inadequate model.
  \vc
  \begin{itemize}
  \item Your model is ignoring some important aspect of the temporal variation
    that is being absorbed by the error terms.
    \vc
  \item Hopefully, you can add the missing component to your model.
  \end{itemize}

  \pagebreak

  Clustering can be viewed as theoretically meaningful or as a nuisance factor
  that just needs to be controlled.
  \vb
  \begin{itemize}
  \item If the clustering is meaningful, you should model the data using
    \emph{multilevel modeling}.
    \vc
    \begin{itemize}
    \item Hierarchical linear regression
      \vc
    \item Mixed models
      \vc
    \item Random effects models
    \end{itemize}
    \vc
  \item If the clustering is an uninteresting nuisance, you can use specialized
    HC variance estimators that deal with clustering.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

<<warning = FALSE>>=
## Read in some data:
LeeBryk <- readRDS(paste0(dataDir, "lee_bryk.rds"))

## Check the data:
str(LeeBryk, vec.len = 3)

## Estimate a linear regression model:
fit <- lm(math ~ ses + sector, data = LeeBryk)

## Calculate the residual ICC:
ICC::ICCbare(x = LeeBryk$schoolid, y = resid(fit))
@

\pagebreak

<<>>=
## Robust tests:
coeftest(fit, vcov = vcovCL(fit, ~ schoolid))

## Raw tests:
summary(fit)$coefficients
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Linearity}

 Each modeled $X$ must exhibit a linear relation with $Y$.
  \begin{itemize}
    \item We can define $X$ via nonlinear transformations of the original data.
  \end{itemize}
  
  \vb

  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, message = FALSE>>=
out2 <- lm(MPG.highway ~ Horsepower, data = Cars93)

p2 <- gg0(x = predict(out2), y = resid(out2))
p3 <- p2 + geom_hline(yintercept = 0, colour = "gray") +
    xlab("Predicted MPG") +
    ylab("Residual MPG")
p4 <- gg0(x = Cars93$Horsepower, y = Cars93$MPG.highway)
p5 <- p4 + geom_smooth(method = "lm", se = FALSE) +
    xlab("Horsepower") +
    ylab("MPG")
p5
@

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, message = FALSE>>=
p3 + geom_smooth(method = "loess", se = FALSE)
@

    \end{column}
  \end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Treating Residual Nonlinearity}

  Nonlinearity in the residual plots is usually a sign of either:
  \begin{enumerate}
  \item Model misspecification
  \item Influential observations
  \end{enumerate}
  \vb
  This type of model misspecification usually implies omitted functions of
  modeled variables.
  \begin{itemize}
  \item Polynomial terms
  \item Interactions
  \end{itemize}
  \vb
  The solution is to include the omitted term into the model and refit.
  \begin{itemize}
  \item This is very much easier said than done.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Limitations of Residual Plots}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      In multiple regression models, basic residual plots won't tell us which 
      predictors exhibit nonlinear associations.

      \vb

<<>>=
out3 <- 
  lm(MPG.highway ~ Horsepower + RPM, 
     data = Cars93)
@

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, message = FALSE>>=
plot(out3, 1)
@

    \end{column}
  \end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Component + Residual Plots}

  \begin{columns}
    \begin{column}{0.5\textwidth}

  We can use \emph{Component + Residual Plots} (AKA, partial residual plots) to
  visualize the unique effects of each $X$ variable.

<<eval = FALSE, message = FALSE>>=
library(car)
crPlots(out3)
@

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, message = FALSE>>=
car::crPlots(out3, layout = c(2, 1))
@

    \end{column}
  \end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Omitted Variables}

  The most common cause of endogeneity (i.e., violating Assumption \ref{exo}) is
  \emph{omitted variable bias}.
  \vb
  \begin{itemize}
  \item If we leave an important predictor variable out of our equation, some
    modeled predictors will become endogenous and their estimated regression
    slopes will be biased.
    \vb
  \item The omitted variable must be correlated with $Y$ and at least one of the
    modeled $X_p$, to be a problem.
  \end{itemize}

  \pagebreak

  Assume the following is the true regression model.
  \begin{align*}
    Y = \beta_0 + \beta_1X + \beta_2Z + \varepsilon
  \end{align*}
  Now, suppose we omit $Z$ from the model:
  \begin{align*}
    Y &= \beta_0 + \beta_1X + \omega\\
    \omega &= \varepsilon + \beta_2Z
  \end{align*}
  Our new error, $\omega$, is a combination of the true error, $\varepsilon$,
  and the omitted term, $\beta_2Z$.
  \begin{itemize}
  \item Consequently, if $X$ and $Z$ are correlated, omitting $Z$ induces a
    correlation between $X$ and $\omega$ (i.e., endogeneity).
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Treating Omitted Variable Bias}

  Omitted variable bias can have severe consequences, but you can't really test
  for it.
  \vb
  \begin{itemize}
  \item The \emph{errors} are correlated with the predictors, but our model is
    estimated under the assumption of exogeneity, so the \emph{residuals} from
    our model will generally be uncorrelated with the predictors.
    \vb
  \item We mostly have to pro-actively work to include all relevant variables in
    our model.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Normality Assumption}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      One of the best ways to evaluate the normality of the error distribution
      with a Q-Q Plot.
      \vc
      \begin{itemize}
      \item Plot the quantiles of the residual distribution against the
        theoretically ideal quantiles.
        \vc
      \item We can actually use a Q-Q Plot to compare any two distributions.
      \end{itemize}

    \end{column}
    \begin{column}{0.5\textwidth}

<<>>=
plot(out1, 2)
@

<<echo = FALSE, eval = FALSE>>=
out1 <- lm(Price ~ Horsepower, data = Cars93)
qqnorm(resid(out1))
qqline(resid(out1))
@

    \end{column}
  \end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Consequences of Violating Normality}

  In small samples, with \emph{\underline{fixed}} predictors, normally
  distributed errors imply normal sampling distributions for the regression
  coefficients.
  \vc
  \begin{itemize}
  \item In large samples, the central limit theorem implies normal sampling
    distributions for the coefficients, regardless of the error distribution.
  \end{itemize}
  \va
  \pause
  Prediction intervals require normally distributed errors.
  \vc
  \begin{itemize}
  \item Confidence intervals for predictions share the same normality
    requirements as the coefficients' sampling distributions.
  \end{itemize}
  \va
  \pause
  Parameter estimates will not be fully efficient.
  \vc
  \begin{itemize}
    \item Standard errors will be larger than they would have been with normally
      distributed errors.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Treating Violations of Normality}

  We usually don't need to do anything about non-normal errors.
  \begin{itemize}
  \item The CLT will protect our inferences.
  \end{itemize}
  \vb
  \pause
  We can use \emph{bootstrapping} to get around the need for normality.
  \begin{enumerate}
  \item Treat your sample as a synthetic population from which you draw many new
    samples (with replacement).
  \item Estimate your model in each new sample.
  \item The replicates of your estimated parameters generate an empirical
    sampling distribution that you can use for inference.
  \end{enumerate}
  \vb
  \pause
  Bootstrapping can be used for inference on pretty much any estimable
  parameter, but it won't work with small samples.
  \begin{itemize}
  \item Need to assume that your sample is representative of the population
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\sectionslide{Influential Observations}

%------------------------------------------------------------------------------%

\begin{frame}{Influential Observations}

  Influential observations contaminate analyses in two ways:
  \vc
  \begin{enumerate}
  \item Exert too much influence on the fitted regression model
    \vc
  \item Invalidate estimates/inferences by violating assumptions
  \end{enumerate}
  \vb
  There are two distinct types of influential observations:
  \vc
  \begin{enumerate}
  \item Outliers
    \vc
    \begin{itemize}
    \item Observations with extreme outcome values, relative to the other data.
      \vc
    \item Observations with outcome values that fit the model very badly.
    \end{itemize}
    \vb
  \item High-leverage observations
    \vc
    \begin{itemize}
    \item Observation with extreme predictor values, relative to other data.
    \end{itemize}
  \end{enumerate}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outliers}

  Outliers can be identified by scrutinizing the residuals.
  \vc
  \begin{itemize}
  \item Observations with residuals of large magnitude may be outliers.
    \vc
  \item The difficulty arises in quantifying what constitutes a ``large''
    residual.
  \end{itemize}
  \vb
  If the residuals do not have constant variance, then we cannot directly
  compare them.
  \vc
  \begin{itemize}
  \item We need to standardize the residuals in some way.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Detecting Outliers}

  We are specifically interested in \emph{externally studentized residuals}.
  \vb
  \begin{itemize}
  \item We can't simply standardize the ordinary residuals.
    \begin{itemize}
    \item \emph{Internally studentized residuals}
      \vc
    \item Outliers can pull the regression line towards themselves.
      \vc
    \item The internally studentized residuals for outliers will be too small.
    \end{itemize}
  \end{itemize}
  \vb
  Begin by defining the concept of a \emph{deleted residual}:
  \begin{align*}
    \hat{\varepsilon}_{(n)} = Y_n - \hat{Y}_{(n)}
  \end{align*}
  \vx{-18}
  \begin{itemize}
    \item $\hat{\varepsilon}_{(n)}$ quantifies the distance of $Y_n$ from the
      regression line estimated after excluding the $n$th observation.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Studentized Residuals}

  If we standardize the deleted residual, $\hat{\varepsilon}_{(n)}$, we get the
  externally studentized residual:
  \begin{align*}
    t_{(n)} = \frac{\hat{\varepsilon}_{(n)}}{SE_{\hat{\varepsilon}_{(n)}}}
  \end{align*}
  The externally studentized residuals have two very useful properties:
  \vb
  \begin{enumerate}
  \item Each $t_{(n)}$ is scaled equivalently.
    \vc
    \begin{itemize}
    \item We can directly compare different $t_{(n)}$.
    \end{itemize}
    \vb
  \item The $t_{(n)}$ are \emph{Student's t} distributed.
    \vc
    \begin{itemize}
    \item We can quantify outliers in terms of quantiles of the $t$
      distribution.
      \vc
    \item $|t_{(n)}| > 3.0$ is a common rule of thumb for flagging outliers.
    \end{itemize}
  \end{enumerate}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Studentized Residual Plots}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      Index plots of the externally studentized residuals can help spotlight
      potential outliers.
      \vb
      \begin{itemize}
      \item Look for observations that clearly ``stand out from the crowd.''
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
      <<>>=
      rstudent(out1) %>% plot()
      @
      
      <<echo = FALSE, eval = FALSE>>=
      p1 <- gg0(x = c(1 : nrow(Cars93)), y = rstudent(out1))
      
      p1 + geom_hline(yintercept = c(-3, 3), colour = "red") +
          xlab("Index") +
          ylab("Externally Studentized Residual MPG")
      @
      
    \end{column}
  \end{columns}
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{High-Leverage Points}

  We identify high-leverage observations through their \emph{leverage} values.
  \vb
  \begin{itemize}
  \item An observation's leverage, $h_n$, quantifies the extent to which its
    predictors affect the fitted regression model.
    \vb
  \item Observations with $X$ values very far from the mean, $\bar{X}$, affect
    the fitted model disproportionately.
  \end{itemize}
  \vb
  \pause
  In simple linear regression, the $n$th leverage is given by:
  \begin{align*}
    h_n = \frac{1}{N} + \frac{\left(X_n - \bar{X}\right)^2}
    {\sum_{m = 1}^N \left(X_{m} - \bar{X}\right)^2}
  \end{align*}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Leverage Plots}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      Index plots of the leverage values can help spotlight high-leverage points.
      \vb
      \begin{itemize}
      \item Again, look for observations that clearly ``stand out from the
        crowd.''
      \end{itemize}

    \end{column}
    \begin{column}{0.5\textwidth}
      
      <<>>=
      hatvalues(out1) %>% plot()
      @
      
      <<echo = FALSE, eval = FALSE>>=
      n <- nrow(Cars93)
      
      p1 <- gg0(x = c(1 : n), y = hatvalues(out1))
      
      p1 + geom_hline(yintercept = 2 / n, colour = "red") +
          xlab("Index") +
          ylab("Leverage Values")
      @
      
    \end{column}
  \end{columns}
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Outliers \& Leverages $\rightarrow$ Influential Points}

  Observations with high leverage or large (externally) studentized residuals
  are not necessarily influential.
  \vc
  \begin{itemize}
  \item High-leverage observations tend to be more influential than outliers.
    \vc
  \item The worst problems arise from observations that are both outliers and
    have high leverage.
  \end{itemize}
  \vb
  \emph{Measures of influence} simultaneously consider extremity in both $X$
  and $Y$ dimensions.
  \vc
  \begin{itemize}
  \item Observations with high measures of influence are very likely to cause
    problems.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Measures of Influence}

  All measures of influence use the same logic as the deleted residual.
  \vc
  \begin{itemize}
  \item Compare models estimated from the whole sample to models estimated from
    samples excluding individual observations.
  \end{itemize}

  \va

 One of the most common measures of influence is \emph{Cook's Distance}.

  \begin{align*}
    \textrm{Cook's} ~ D_n &= \frac{\sum_{n = 1}^N \left( \hat{Y}_n - \hat{Y}_{(n)} \right)^2}{\left(P + 1\right) \hat{\sigma}^2}\\[6pt]
                          &= (P + 1)^{-1} t_n^2 \frac{h_n}{1 - h_n}
  \end{align*}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Plots of Cook's Distance}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      Index plots of Cook's distances can help spotlight the influential points.
      \vb
      \begin{itemize}
      \item Look for observations that clearly ``stand out from the crowd.''
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
      <<echo = FALSE, eval = FALSE>>=
      fCrit <- qf(0.5, 2, n - 2)
      
      p1 <- gg0(x = c(1 : n), y = cooks.distance(out1))
      
      p1 + geom_hline(yintercept = c(fCrit), colour = "red") +
          xlab("Index") +
          ylab("Cook's Distance")
      @
      
      <<>>=
      cd <- cooks.distance(out1)
      plot(cd)
      @
      
    \end{column}
  \end{columns}
  
\end{frame}

%------------------------------------------------------------------------------%

\subsection{Treating Influential Observations}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Removing Influential Observations}
  
  <<>>=
  (maxD <- which.max(cd))
  @
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      Observation number \Sexpr{maxD} was the most influential
      according to Cook's Distance.
      \vb
      \begin{itemize}
      \item Removing that observation has a small impact on the fitted
        regression line.
        \vc
      \item Influential observations don't only affect the regression line,
        though.
      \end{itemize}

    \end{column}
    \begin{column}{0.5\textwidth}
      
      <<echo = FALSE, message = FALSE>>=
      x <- Cars93$Horsepower
      y <- Cars93$Price
      
      x2 <- x[-maxD]
      y2 <- y[-maxD]
      
      colVec <- rep("black", nrow(Cars93))
      colVec[maxD] <- "red"
      
      p1 <- gg0(x = x, y = y, points = FALSE) +
          geom_point(aes(colour = colVec), show.legend = FALSE, size = 2) +
          xlab("Horsepower") +
          ylab("Price")
      
      p1 + geom_smooth(method = "lm", se = FALSE, linewidth = 2) +
          geom_smooth(mapping   = aes(x = x2, y = y2),
                      method    = "lm",
                      se        = FALSE,
                      color     = "red",
                      linetype  = "dashed",
                      linewidth = 2) +
          scale_colour_manual(values = c("black", "red"))
      @
      
    \end{column}
  \end{columns}
  
\end{frame}

% ------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Removing Influential Observations}
  
  <<>>=
  ## Exclude the influential case:
  Cars93.2 <- Cars93[-maxD, ]
  
  ## Fit model with reduced sample:
  out2 <- lm(Price ~ Horsepower, data = Cars93.2)
  
  round(summary(out1)$coefficients, 6)
  round(summary(out2)$coefficients, 6)
  @
  
  \pagebreak
  
  <<>>=
  partSummary(out1, 2)
  partSummary(out2, 2)
  @
  
  \pagebreak
  
  <<>>=
  summary(out1)[c("sigma", "r.squared", "fstatistic")] %>%
      unlist() %>%
      head(3)
  summary(out2)[c("sigma", "r.squared", "fstatistic")] %>%
      unlist() %>%
      head(3)
  @
  
\end{frame}

% ------------------------------------------------------------------------------%

\begin{frame}[fragile]{Removing Influential Observations}
  
  <<>>=
  (maxDs <- sort(cd) %>% names() %>% tail(2) %>% as.numeric())
  @
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      If we remove the two most influential observations, \Sexpr{maxDs[1]} and
      \Sexpr{maxDs[2]}, the fitted regression line barely changes at all.
      \vc
      \begin{itemize}
      \item The influences of these two observations were counteracting one
        another.
      \item We're probably still better off, though.
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
      <<echo = FALSE, message = FALSE>>=
      x <- Cars93$Horsepower
      y <- Cars93$Price
      
      x2 <- x[-maxDs]
      y2 <- y[-maxDs]
      
      colVec <- rep("black", nrow(Cars93))
      colVec[maxDs] <- "red"
      
      p1 <- gg0(x = x, y = y, points = FALSE) +
          geom_point(aes(colour = colVec), show.legend = FALSE, size = 2) +
          xlab("Horsepower") +
          ylab("Price")
      
      p1 + geom_smooth(method = "lm", se = FALSE, linewidth = 2) +
          geom_smooth(mapping   = aes(x = x2, y = y2),
                      method    = "lm",
                      se        = FALSE,
                      color     = "red",
                      linetype  = "dashed",
                      linewidth = 2) +
          scale_colour_manual(values = c("black", "red"))
      @
      
    \end{column}
  \end{columns}
  
\end{frame}

% ------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Removing Influential Observations}
  
  <<>>=
  ## Exclude influential cases:
  Cars93.2 <- Cars93[-maxDs, ]
  
  ## Fit model with reduced sample:
  out2.2 <- lm(Price ~ Horsepower, data = Cars93.2)
  
  round(summary(out1)$coefficients, 6)
  round(summary(out2.2)$coefficients, 6)
  @
  
  \pagebreak
  
  <<>>=
  partSummary(out1, 2)
  partSummary(out2.2, 2)
  @
  
  \pagebreak
  
  <<>>=
  summary(out1)[c("sigma", "r.squared", "fstatistic")] %>%
      unlist() %>%
      head(3)
  summary(out2.2)[c("sigma", "r.squared", "fstatistic")] %>%
      unlist() %>%
      head(3)
  @
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Treating Influential Points}
  
  The most common way to address influential observations is simply to delete
  them and refit the model.
  \vc
  \begin{itemize}
  \item This approach is often effective---and always simple---but it is not
    fool-proof.
    \vc
  \item Although an observation is influential, we may not be able to justify
    excluding it from the analysis.
  \end{itemize}
  \vb
  Robust regression procedures can estimate the model directly in the
  presence of influential observations.
  \vc
  \begin{itemize}
  \item Observations in the tails of the distribution are weighted less in the
    estimation process, so outliers and high-leverage points cannot exert
    substantial influence on the fit.
    \vc
  \item We can do robust regression with the \src{rlm()} function from the
    \pkg{MASS} package.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\end{document}
