%%% Title:    GLM and Logistic Regression
%%% Author:   Kyle M. Lang
%%% Created:  2017-09-12
%%% Modified: 2021-11-25

\documentclass[10pt]{beamer}
\usetheme{Utrecht}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{booktabs}
\usepackage{fancybox}
\usepackage{caption}

\newcommand{\kfold}[0]{\emph{K}-fold cross-validation}

\captionsetup[table]{labelformat = empty}

\title{Generalized Linear Model \& Logistic Regression}
\subtitle{Fundamental Techniques in Data Science}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Utrecht University}
\date{}

\begin{document}

<<setup, include=FALSE>>=
set.seed(235711)

                                        #install.packages("mvtnorm", repos = "http://cloud.r-project.org", depends = TRUE)

library(knitr)
library(ggplot2)
library(MASS)
library(mvtnorm)
library(xtable)
library(MLmetrics)
library(caret)
library(pROC)

source("../../../code/supportFunctions.R")
source("../../../code/xtable.confusionMatrix.R")

opts_chunk$set(size = 'footnotesize', fig.align = 'center', message = FALSE)
knit_theme$set('edit-kwrite')

lightBlue <- rgb(0, 137, 191, max = 255)
midBlue   <- rgb(0, 131, 183, max = 255)
darkBlue  <- rgb(0, 128, 179, max = 255)
deepGold  <- rgb(184, 138, 45, max = 255)
lightGold <- rgb(195, 146, 48, max = 255)
@

%------------------------------------------------------------------------------%

\begin{frame}[t,plain]
  \titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{General Linear Model}

  So far, we've been discussing models with this form:
  \begin{align*}
    Y = \beta_0 + \sum_{p = 1}^P \beta_p X_p + \varepsilon
  \end{align*}
  This type of model is known as the \emph{general linear model}.
  \vc
  \begin{itemize}
  \item All flavors of linear regression are general linear models.
    \vc
    \begin{itemize}
    \item ANOVA
    \item ANCOVA
    \item Multilevel linear regression models
    \end{itemize}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Components of the General Linear Model}

  We can break our model into pieces:
  \begin{align*}
    \eta &= \beta_0 + \sum_{p = 1}^P \beta_p X_p\\
    Y &= \eta + \varepsilon
  \end{align*}
  Because $\varepsilon \sim \mathrm{N}(0, \sigma^2)$, we can also write:
  \begin{align*}
    Y \sim \mathrm{N}(\eta, \sigma^2)
  \end{align*}
  In this representation:
  \begin{itemize}
  \item $\eta$ is the \emph{systematic component} of the model
  \item The normal distribution, $\mathrm{N}(\cdot, \cdot)$, is the model's
    \emph{random component}.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Components of the General Linear Model}

  The purpose of general linear modeling (i.e., regression modeling) is to build
  a model of the outcome's mean, $\mu_Y$.
  \begin{itemize}
  \item In this case, $\mu_Y = \eta$.
  \item The systematic component defines the mean of $Y$.
  \end{itemize}
  \vb
  The random component quantifies variability (i.e., error variance) around
  $\mu_Y$.
  \begin{itemize}
  \item In the general linear model, we assume that this error variance follows
    a normal distribution.
  \item Hence the normal random component.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\sectionslide{Generalized Linear Model}

%------------------------------------------------------------------------------%

\begin{frame}{Extending the General Linear Model}

  We can generalize the models we've been using in two important ways:
  \vc
  \begin{enumerate}
  \item Allow for random components other than the normal distribution.
    \vc
  \item Allow for more complicated relations between $\mu_Y$ and $\eta$.
    \begin{itemize}
    \item Allow: $g(\mu_Y) = \eta$
    \end{itemize}
  \end{enumerate}
  \vc
  These extensions lead to the class of \emph{generalized linear models} (GLMs).

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Components of the Generalized Linear Model}

  The random component in a GLM can be any distribution from the so-called
  \emph{exponential family}.
  \vc
  \begin{itemize}
  \item The exponential family contains many popular distributions:
    \vc
    \begin{itemize}
    \item Normal
    \item Binomial
    \item Poisson
    \item Many others...
    \end{itemize}
  \end{itemize}
  \vc
  The systematic component of a GLM is exactly the same as it is in general
  linear models:
  \begin{align*}
    \eta = \beta_0 + \sum_{p = 1}^P \beta_p X_p
  \end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Link Functions}

  In GLMs, $\eta$ does not directly describe $\mu_Y$.
  \begin{itemize}
  \item We first transform $\mu_Y$ via a \emph{link function}.
  \item $g(\mu_Y) = \eta$
  \end{itemize}
  \vb
  The link function allows GLMs for outcomes with restricted ranges without
  requiring any restrictions on the range of the $\{X_p\}$.
  \vc
  \begin{itemize}
  \item For strictly positive $Y$, we can use a \emph{log link}:
    \begin{align*}
      \ln(\mu_y) = \eta.
    \end{align*}
  \item The general linear model employs the \emph{identity link}:
    \begin{align*}
      \mu_y = \eta.
    \end{align*}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Components of the Generalized Linear Model}

  Every GLM is built from three components:
  \vb
  \begin{enumerate}
  \item The systematic component, $\eta$.
    \begin{itemize}
    \item A linear function of the predictors, $\{X_p\}$.
    \item Describes the association between $\mathbf{X}$ and $Y$.
    \end{itemize}
    \vb
  \item The link function, $g(\mu_Y)$.
    \begin{itemize}
    \item Transforms $\mu_Y$ so that it can take any value on the real line.
    \end{itemize}
    \vb
  \item The random component, $P(Y|g^{-1}(\eta))$
    \begin{itemize}
    \item The distribution of the observed $Y$.
    \item Quantifies the error variance around $\eta$.
    \end{itemize}
  \end{enumerate}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{General Linear Model $\subset$ Generalized Linear Model}

  The general linear model is a special case of GLM.
  \vb
  \begin{enumerate}
  \item Systematic component:
    \begin{align*}
      \eta = \beta_0 + \sum_{p = 1}^P \beta_p X_p
    \end{align*}
  \item Link function:
    \begin{align*}
      \mu_Y = \eta
    \end{align*}
  \item Random component:
    \begin{align*}
      Y \sim \mathrm{N}(\eta, \sigma^2)
    \end{align*}
  \end{enumerate}

\end{frame}

%------------------------------------------------------------------------------%

\sectionslide{Logistic Regression}

%------------------------------------------------------------------------------%

\begin{frame}{Logistic Regression}

  So why do we care about the GLM when linear regression models have worked
  thus far?
  \begin{itemize}
  \item In a word: Classification.
  \end{itemize}
  \vb
  In the classification task, we have a discrete, qualitative outcome.
  \begin{itemize}
  \item We will begin with the situation of two-level outcomes.
    \begin{itemize}
    \item Alive or Dead
    \item Pass or Fail
    \item Pay or Default
    \end{itemize}
  \end{itemize}
  \vb
  We want to build a model that predicts class membership based on some set of
  interesting features.
  \begin{itemize}
  \item To do so, we will use a very useful type of GLM: \emph{logistic
    regression}.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Classification Example}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      Suppose we want to know the effect of study time on the probability of
      passing an exam.
      \vc
      \begin{itemize}
      \item The probability of passing must be between 0 and 1.
        \vc
      \item We care about the probability of passing, but we only observe
        absolute success or failure.
        \vc
        \begin{itemize}
        \item $Y \in \{1, 0\}$
        \end{itemize}
      \end{itemize}

      \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
beta <- c(-5.5, 1)
x    <- runif(100, 0, 12)
eta  <- beta[1] + beta[2] * x
pi   <- exp(eta) / (1 + exp(eta))
y    <- rbinom(100, 1, pi)

dat1 <- data.frame(x, y)

p1 <- gg0(x = dat1$x, y = dat1$y) +
    xlab("Hours of Study") +
    ylab("Probability of Passing")
p1
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Linear Regression for Binary Outcomes?}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      What happens if we try to model these data with linear regression?
      \vc
      \begin{itemize}
      \item Hmm...notice any problems?
      \end{itemize}

    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p1 + geom_smooth(method = "lm", se = FALSE)
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Logistic Regression Visualized}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      We get a much better model using logistic regression.
      \vc
      \begin{itemize}
      \item The link function ensures legal predicted values.
        \vc
      \item The sigmoidal curve implies fluctuation in the effectiveness of
        extra study time.
        \vc
        \begin{itemize}
        \item More study time is most beneficial for students with around
          \Sexpr{round(-beta[1] / beta[2], 2)} hours of study.
        \end{itemize}
      \end{itemize}

    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
tmp  <- with(dat1, seq(min(x), max(x), length.out = 500))
eta2 <- beta[1] + beta[2] * tmp
pi2  <- exp(eta2) / (1 + exp(eta2))
dat2 <- data.frame(x = tmp, y = pi2)

p1 + geom_line(aes(x = x, y = y), data = dat2, colour = "blue", size = 1)
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Defining the Logistic Regression Model}

  In logistic regression problems, we are modeling binary data:
  \begin{itemize}
  \item Usual coding: $Y \in \{1 = \text{``Success''}, 0 = \text{``Failure''} \}$.
  \end{itemize}
  \vb
  The \emph{Binomial} distribution is a good way to represent this kind of data.
  \begin{itemize}
  \item The systematic component in our logistic regression model will be the
    binomial distribution.
  \end{itemize}
  \vb
  The mean of the binomial distribution (with $N = 1$) is the ``success''
  probability, $\pi = P(Y = 1)$.
  \begin{itemize}
  \item We are interested in modeling $\mu_Y = \pi$:
    \begin{align*}
      g(\pi) = \beta_0 + \sum_{p = 1}^P \beta_p X_p
    \end{align*}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Link Function for Logistic Regression}

  Because $\pi$ is bounded by 0 and 1, we cannot model it directly---we must
  apply an appropriate link function.
  \vc
  \begin{itemize}
  \item Logistic regression uses the \emph{logit link}.
    \vc
  \item Given $\pi$, we can define the \emph{odds} of success as:
    \begin{align*}
      O_s = \frac{\pi}{1 - \pi}
    \end{align*}
  \item Because $\pi \in [0, 1]$, we know that $O_s \geq 0$.
    \vc
  \item We take the natural log of the odds as the last step to fully map
    $\pi$ to the real line.
    \begin{align*}
      \mathrm{logit}(\pi) = \ln \left(\frac{\pi}{1 - \pi}\right)
    \end{align*}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Fully Specified Logistic Regression Model}

  Our final logistic regression model is:
  \begin{align*}
    Y &\sim \mathrm{Bin}(\pi, 1)\\
    \mathrm{logit}(\pi) &= \beta_0 + \sum_{p = 1}^P \beta_p X_p
  \end{align*}
  The fitted model can be represented as:
  \begin{align*}
    \mathrm{logit}(\hat{\pi}) = \hat{\beta}_0 + \sum_{p = 1}^P \hat{\beta}_p X_p
  \end{align*}
  The fitted coefficients, $\{\hat{\beta}_0, \hat{\beta}_p\}$, are interpreted
  in units of \emph{log odds}.

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Logistic Regression Example}

<<echo = FALSE>>=
out1 <- glm(y ~ x, data = dat1, family = binomial())

b0 <- round(coef(out1)[1], 3)
b1 <- round(coef(out1)[2], 3)

b0e <- round(exp(coef(out1))[1], 3)
b1e <- round(exp(coef(out1))[2], 3)
@

If we fit a logistic regression model to the test-passing data plotted above, we
get:
\begin{align*}
  \mathrm{logit}(\hat{\pi}_{pass}) = \Sexpr{b0} + \Sexpr{b1} X_{study}
\end{align*}
\vx{-12}
\begin{itemize}
\item A student who does not study at all has \Sexpr{b0} log odds of passing the
  exam.
\item For each additional hour of study, a student's log odds of passing increase
  by \Sexpr{b1} units.
\end{itemize}
\vb
Log odds do not lend themselves to interpretation.
\begin{itemize}
\item We can convert the effects back to an odds scale by exponentiation.
\item $\hat{\beta}$ has log odds units, but $e^{\hat{\beta}}$ has odds units.
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Interpretations}

  Exponentiating the coefficients also converts the additive effects to
  multiplicative effects.
  \vc
  \begin{itemize}
  \item $\ln(AB) = \ln(A) + \ln(B)$
  \item We can interpret $\hat{\beta}$ as we would in linear regression:
    \begin{itemize}
    \item A unit change in $X_p$ produces an expected change of $\hat{\beta}_p$
      units in $\mathrm{logit}(\pi)$.
    \end{itemize}
    \vc
  \item After exponentiation, however, unit changes in $X_p$ imply multiplicative
    changes in $O_s = \pi / (1 - \pi)$.
    \begin{itemize}
    \item A unit change in $X_p$ results in multiplying $O_s$  by
      $e^{\hat{\beta}_p}$.
    \end{itemize}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Interpretations}

  Exponentiating the coefficients in our toy test-passing example produces the
  following interpretations:
  \begin{itemize}
  \item A student who does not study is expected to pass the exam with odds of
    \Sexpr{b0e}.
  \item For each additional hour a student studies, their odds of passing
    increase by \Sexpr{b1e} \emph{times}.
    \begin{itemize}
    \item Odds of passing are \emph{multiplied} by \Sexpr{b1e} for each extra
      hour of study.
    \end{itemize}
  \end{itemize}

  \vb
  \pause

  Due to the confusing interpretations of the coefficients, we often focus
  on the valance of the effects:
  \begin{itemize}
  \item Additional study time is associated with increased odds of passing.
  \item $\hat{\beta_p} > 0$ = ``Increased Success'',  $e^{\hat{\beta}_p} > 1$ =
    ``Increased Success''
  \end{itemize}
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Multiple Logistic Regression}

  The preceding example was a \emph{simple logistic regression}.
  \vc
  \begin{itemize}
  \item Including multiple predictor variables in the systematic component leads
    to \emph{multiple logistic regression}.
    \vc
  \item The relative differences between simple logistic regression and multiple
    logistic regression are the same as those between simple linear regression
    and multiple linear regression.
    \vc
    \begin{itemize}
    \item The only important complication is that the regression coefficients
      become partial effects.
    \end{itemize}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Multiple Logistic Regression Example}

<<echo = FALSE>>=
diabetes <- readRDS("../data/diabetes.rds")

diabetes$highGlu <- as.numeric(diabetes$glu > 90)

diabetes$age40 <- diabetes$age - 40
diabetes$bmi25 <- diabetes$bmi - 25
diabetes$bp100 <- diabetes$bp - 100

out1 <-
    glm(highGlu ~ age40 + bmi25 + bp100, data = diabetes, family = binomial())

beta1  <- coef(out1)
eBeta1 <- exp(beta1)

b0 <- round(beta1[1], 3)
b1 <- round(beta1[2], 3)
b2 <- round(beta1[3], 3)
b3 <- round(beta1[4], 3)

eB0 <- round(eBeta1[1], 3)
eB1 <- round(eBeta1[2], 3)
eB2 <- round(eBeta1[3], 3)
eB3 <- round(eBeta1[4], 3)
@

Suppose we want to predict the probability of a patient having ``high'' blood
glucose from their age, BMI, and average blood pressure.
\vc
\begin{itemize}
\item We could do so with the following model:
  \begin{align*}
    \mathrm{logit}(\pi_{hi.gluc}) =
    \beta_0 + \beta_1 X_{age.40} + \beta_2 X_{BMI.25} + \beta_3 X_{BP.100}
  \end{align*}
\item By fitting this model to our usual ``diabetes'' data we get:
  \begin{align*}
    \mathrm{logit}(\hat{\pi}_{hi.gluc}) =
    \Sexpr{b0} + \Sexpr{b1} X_{age.40} + \Sexpr{b2} X_{BMI.25} + \Sexpr{b3} X_{BP.100}
  \end{align*}
\item Exponentiating the coefficients produces:
  \begin{align*}
    \frac{\hat{\pi}_{hi.gluc}}{1 - \hat{\pi}_{hi.gluc}} =
    \Sexpr{eB0} \times \Sexpr{eB1}^{X_{age.40}} \times \Sexpr{eB2}^{X_{BMI.25}} \times
    \Sexpr{eB3}^{X_{BP.100}}
  \end{align*}
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Exponentiating the Systematic Component}

  \begin{align*}
    \mathrm{logit}(\hat{\pi}_{hi.gluc}) &= \Sexpr{b0} + \Sexpr{b1} X_{age.40} +
    \Sexpr{b2} X_{BMI.25} + \Sexpr{b3} X_{BP.100}\\[12pt]
    e^{\mathrm{logit}(\hat{\pi}_{hi.gluc})} &=
    e^{\left(\Sexpr{b0} ~ + ~ \Sexpr{b1} X_{age.40} ~ + ~ \Sexpr{b2} X_{BMI.25}
      ~ + ~ \Sexpr{b3} X_{BP.100} \right)}\\[8pt]
    \frac{\hat{\pi}_{hi.gluc}}{1 - \hat{\pi}_{hi.gluc}} &=
    e^{\Sexpr{b0}} \times e^{\Sexpr{b1} X_{age.40}} \times e^{\Sexpr{b2} X_{BMI.25}} \times
    e^{\Sexpr{b3} X_{BP.100}}\\[8pt]
    &= \left(e^{\Sexpr{b0}}\right) \times \left(e^{\Sexpr{b1}}\right)^{X_{age.40}} \times
    \left(e^{\Sexpr{b2}}\right)^{X_{BMI.25}} \times
    \left(e^{\Sexpr{b3}}\right)^{X_{BP.100}}\\[14pt]
    &= \Sexpr{eB0} \times \Sexpr{eB1}^{X_{age.40}} \times \Sexpr{eB2}^{X_{BMI.25}}
    \times \Sexpr{eB3}^{X_{BP.100}}
  \end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\sectionslide{Classification}

%------------------------------------------------------------------------------%

\begin{frame}{Predictions from Logistic Regression}

  Given a fitted logistic regression model, we can get predictions for new
  observations of $\{X_p\}$, $\{X_p'\}$.
  \vc
  \begin{itemize}
  \item Directly applying $\{\hat{\beta}_0, \hat{\beta}_p\}$ to $\{X_p'\}$ will
    produce predictions on the scale of $\eta$:
    \begin{align*}
      \hat{\eta}' = \hat{\beta}_0 + \sum_{p = 1}^P \hat{\beta}_p X_p'
    \end{align*}
  \item By applying the inverse link function, $g^{-1}(\cdot)$, to
    $\hat{\eta}'$, we get predicted success probabilities:
    \begin{align*}
      \hat{\pi}' = g^{-1}(\hat{\eta}')
    \end{align*}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Predictions from Logistic Regression}

  In logistic regression, the inverse link function, $g^{-1}(\cdot)$, is the
  \emph{logistic function}:
  \begin{align*}
    \mathrm{logistic}(X) = \frac{e^X}{1 + e^X}
  \end{align*}
  So, we convert $\hat{\eta}'$ to $\hat{\pi}'$ by:
  \begin{align*}
    \hat{\pi}' &= \frac{e^{\hat{\eta}'}}{1 + e^{\hat{\eta}'}} = \frac{\exp \left(
      \hat{\beta}_0 + \sum_{p = 1}^P \hat{\beta}_p X_p' \right) }{1 + \exp \left(
      \hat{\beta}_0 + \sum_{p = 1}^P \hat{\beta}_p X_p' \right) }
  \end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Classification with Logistic Regression}

  Once we have computed the predicted success probabilities, $\hat{\pi}'$, we
  can use them to classify new observations.
  \vc
  \begin{itemize}
  \item By choosing a threshold on $\hat{\pi}'$, say $\hat{\pi}' = t$, we can
    classify the new observations as ``Successes'' or ``Failures'':
    \begin{align*}
      \hat{Y}' = \left\{
      \begin{array}{ccc}
        1 & if & \hat{\pi}' \geq t\\
        0 & if & \hat{\pi}' < t\\
      \end{array}
      \right.
    \end{align*}

  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Classification Example}

<<echo = FALSE>>=
xn   <- c(1, 57 - 40, 28 - 25, 92 - 100)
eta1 <- crossprod(xn, beta1)
pi1  <- round(exp(eta1) / (1 + exp(eta1)), 3)
eta1 <- round(eta1, 3)
g    <- as.numeric(pi1 > 0.5) + 1
@

Say we want to classify a new patient into either the ``high glucose'' group
or the ``not high glucose'' group using the model fit above.
\begin{itemize}
\item Assume this patient has the following characteristics:
  \begin{itemize}
  \item They are 57 years old
  \item Their BMI is 28
  \item Their average blood pressure is 92
  \end{itemize}
\end{itemize}
\vb
First we plug their predictor data into the fitted model to get their
model-implied $\eta$:
\begin{align*}
  \hat{\eta} &= \Sexpr{b0} + \Sexpr{b1} (57 - 40) + \Sexpr{b2} (28 - 25) + \Sexpr{b3} (92 - 100)\\
  &= \Sexpr{eta1}
\end{align*}

\pagebreak

Next we convert the predicted $\eta$ value into a model-implied success
probability by applying the logistic function:
\begin{align*}
  \frac{e^{\Sexpr{eta1}}}{1 + e^{\Sexpr{eta1}}} = \Sexpr{pi1}
\end{align*}\\
\vb
Finally, to make the classification, assume a threshold of $\hat{\pi}' = 0.5$ as
the decision boundary.
\begin{itemize}
\item Because $\Sexpr{pi1} \Sexpr{c("<", ">")[g]} 0.5$ we would classify this
  patient into the ``\Sexpr{c("low", "high")[g]} glucose'' group.
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\sectionslide{Evaluating Classification Performance}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Confusion Matrix}

  One of the most direct ways to evaluate classification performance is to
  tabulate the true and predicted classes.
  \begin{itemize}
  \item Such a cross-tabulation is called a \emph{confusion matrix}.
  \end{itemize}

<<echo = FALSE, results = "asis">>=
yTrue <- factor(diabetes$highGlu, labels = c("Low", "High"))
yPred <- factor(predict(out1, type = "response") > 0.5, labels = c("Low", "High"))

tab  <- table(True = yTrue, Predicted = yPred)
xTab <- xtable(tab, caption = "Confusion Matrix of Blood Glucose Level", digits = 0)

adds <- list(pos = list(0, 0),
             command = c("& \\multicolumn{2}{c}{Predicted} \\\\\n",
                         "True & Low & High \\\\\n")
             )

print(xTab, add.to.row = adds, include.colnames = FALSE)
@

\pause

We can summarize the confusion matrix in many ways.
\begin{itemize}
\item Different summaries highlight different aspects of the classifier's
  performance.
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Summarizing the Confusion Matrix}

  Sensitivity (Recall, Hit Rate, True-Positive Rate):
  \begin{align*}
    Sensitivity = \frac{True~Postives}{True~Positives + False~Negatives} = \frac{True~Postives}{Total~Positives}
  \end{align*}\\

  \va

  Specificity (Selectivity, True-Negative Rate):
  \begin{align*}
    Specificity = \frac{True~Negatives}{True~Negatives + False~Positives} = \frac{True~Negatives}{Total~Negatives}
  \end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Summarizing the Confusion Matrix}

  Accuracy:
  \begin{align*}
    Accuracy = \frac{True~Positives + True~Negatives}{TP + TN + FP + FN} = \frac{Correct~Classifications}{Total~Cases}
  \end{align*}\\

\va

  Error Rate:
  \begin{align*}
    Error~Rate = \frac{False~Positives + False~Negatives}{TP + TN + FP + FN} = \frac{Incorrect~Classifications}{Total~Cases}
  \end{align*}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{ROC Curve}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      We can visualize a classifier's performance via a \emph{Receiver
        Operating Characteristic} Curve.
      \vb
      \begin{itemize}
      \item Y-Axis: True-Positive Rate
        \begin{itemize}
        \item Sensitivity
        \end{itemize}
        \vb
      \item X-Axis: False-Positive Rate
        \begin{itemize}
        \item 1 - Specificity
        \end{itemize}
        \vb
      \item Area Under the ROC Curve (AUC) summarizes the classifier's
        discrimination
      \end{itemize}

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
true <- rbinom(100, 1, 0.5)
pred <- true + rnorm(100, 0, 0.5)

roc0 <- roc(response = true, predictor = pred)

plot(roc0, legacy.axes = TRUE, main = "ROC Curve")
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Example}

<<echo = FALSE>>=
roc0 <- roc(response = diabetes$highGlu,
            predictor = predict(out1, type = "response")
            )
@

  \begin{columns}
    \begin{column}{0.5\textwidth}

 \begin{alignat*}{2}
  Sensitivity &= \frac{\Sexpr{tab[2, 2]}}{\Sexpr{tab[2, 2]} + \Sexpr{tab[2, 1]}} &&=
  \Sexpr{round(tab[2, 2] / sum(tab[2, ]), 3)}\\[8pt]
   Specificity &= \frac{\Sexpr{tab[1, 1]}}{\Sexpr{tab[1, 1]} + \Sexpr{tab[1, 2]}} &&=
  \Sexpr{round(tab[1, 1] / sum(tab[1, ]), 3)}\\[8pt]
   Accuracy &= \frac{\Sexpr{tab[2, 2]} + \Sexpr{tab[1, 1]}}{\Sexpr{tab[2, 2]} +
   \Sexpr{tab[1, 1]} + \Sexpr{tab[2, 1]} + \Sexpr{tab[1, 2]}} &&=
  \Sexpr{round((tab[2, 2] + tab[1, 1])/ sum(tab), 3)}\\[8pt]
   Error~Rate &= \frac{\Sexpr{tab[2, 1]} + \Sexpr{tab[1, 2]}}{\Sexpr{tab[2, 2]} +
   \Sexpr{tab[1, 1]} + \Sexpr{tab[2, 1]} + \Sexpr{tab[1, 2]}} &&=
  \Sexpr{round((tab[2, 1] + tab[1, 2])/ sum(tab), 3)}\\[8pt]
  AUC &&&= \Sexpr{round(auc(roc0), 3)}
\end{alignat*}

  \end{column}
  \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
plot(roc0, legacy.axes = TRUE, main = "ROC of Blood Glucose Classifier")
@

    \end{column}
  \end{columns}

\end{frame}

%------------------------------------------------------------------------------%
\comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Classification Error}

  The MSE is not an appropriate error measure for classification.
  \begin{itemize}
  \item The differences between predicted and observed outcomes have little
    meaning.
  \end{itemize}
  \vb
  One of the most popular error measures is the \emph{Cross-Entropy Error}:
  \begin{align*}
    CEE = -N^{-1} \sum_{n = 1}^N Y_n \ln(\hat{\pi}_n) + (1 - Y_n)\ln(1 - \hat{\pi}_n)
  \end{align*}
  \vx{-6}
  \begin{itemize}
  \item The CEE is sensitive to classification confidence.
  \item Stronger predictions are more heavily weighted.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Why not Accuracy?}

<<echo = FALSE>>=
yTrue <- rbinom(100, 1, 0.5)

pi1   <- yTrue * 0.9 + (1 - yTrue) * 0.1
pred1 <- as.numeric(pi1 > 0.5)

pi2   <- yTrue * 0.55 + (1 - yTrue) * 0.45
pred2 <- as.numeric(pi2 > 0.5)

cee1 <- LogLoss(y_pred = pi1, y_true = yTrue)
cee2 <- LogLoss(y_pred = pi2, y_true = yTrue)
@

 The accuracy is a na\"{i}vely appealing option.
  \begin{itemize}
  \item The proportion of cases assigned to the correct group
  \end{itemize}
  \vb
  Consider two perfect classifiers:
  \begin{enumerate}
  \item $P(\hat{Y}_n = 1 | Y_n = 1) = 0.90$,  $P(\hat{Y}_n = 1 | Y_n = 0) = 0.10$, $n = 1, 2, \ldots, N$
  \item $P(\hat{Y}_n = 1 | Y_n = 1) = 0.55$,  $P(\hat{Y}_n = 1 | Y_n = 0) = 0.45$, $n = 1, 2, \ldots, N$
  \end{enumerate}
  \vb
  Both of these classifiers will have the same accuracy.
  \begin{itemize}
  \item Neither model ever makes an incorrect group assignment.
  \end{itemize}
  \vb
  The first model will have a lower CEE.
  \begin{itemize}
  \item The classifications are made with higher confidence.
  \item $CEE_1 = \Sexpr{round(cee1, 3)}$, $CEE_2 = \Sexpr{round(cee2, 3)}$
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Conclusion}

  \begin{itemize}
  \item The Generalized Linear Model is a flexible class of models that we can
    use for non-normally distributed outcomes.
    \begin{itemize}
    \item Multiple linear regression is a special type of GLM.
    \end{itemize}
    \vc
  \item We cannot model nominal outcomes with linear regression.
    \begin{itemize}
    \item We should use some form of logistic regression.
    \end{itemize}
    \vc
  \item We use logistic regression for binary outcomes and multinomial logistic
    regression for multi-class nominal outcomes.
    \vc
  \item We must take care when interpreting the coefficients from logistic
    regression models.
    \vc
  \item We can use the estimated success probabilities from a fitted model to
    classify new observations.
  \end{itemize}

\end{frame}
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------------------------------------------------------------------------------%

\end{document}

