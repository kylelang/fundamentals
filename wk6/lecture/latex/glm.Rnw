%%% Title:    GLM and Logistic Regression
%%% Author:   Kyle M. Lang
%%% Created:  2017-09-12
%%% Modified: 2024-12-16

\documentclass[10pt]{beamer}
\usetheme{Utrecht}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{booktabs}
\usepackage{fancybox}
\usepackage{caption}
\usepackage{listings}
\usepackage{xspace}
\usepackage{tcolorbox}
\tcbuselibrary{listings}
\usepackage{hyperref}

\newcommand{\kfold}[0]{\emph{K}-fold cross-validation}

\definecolor{codebackground}{RGB}{224,234,238}
\definecolor{codestring}{RGB}{191,3,3}
\definecolor{codekeyword}{RGB}{1,1,129}
\definecolor{codecomment}{RGB}{131,129,131}

\newcommand{\src}[1]{%
  \tcbox[%
  on line,
  colback = codebackground,
  colframe = codebackground,
  left = 0pt,
  right = 0pt,
  top = 0pt,
  bottom = 0pt%
  ]{%
    \lstinline[%
    language = R,
    basicstyle = \ttfamily,
    keywordstyle = \color{codekeyword},
    commentstyle = \color{codecomment}\itshape,
    stringstyle = \color{codestring},
    deletekeywords = {_}
    % frame = single,
    % frameround = tttt,
    % fillcolor = \color{blue}%
    ]{#1}%
  }
}

\newcommand{\rmsc}[1]{\textrm{\textsc{#1}}}
\newcommand{\pkg}[1]{\textbf{#1}}

\captionsetup[table]{labelformat = empty}

\title{Generalized Linear Model \& Logistic Regression}
\subtitle{Fundamental Techniques in Data Science}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Utrecht University}
\date{}

\begin{document}

<<setup, include = FALSE>>=
set.seed(235711)

library(knitr)
library(ggplot2)
library(xtable)
library(MLmetrics)
library(dplyr)
                                        #library(caret)
                                        #library(pROC)
dataDir <- "../../../data/"

source("../../../code/supportFunctions.R")

opts_chunk$set(size = 'footnotesize', fig.align = 'center', message = FALSE)
knit_theme$set('edit-kwrite')
@

%------------------------------------------------------------------------------%

\begin{frame}[t,plain]
  \titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{General Linear Model}

  So far, we've been discussing models with this form:
  \begin{align*}
    Y = \beta_0 + \sum_{p = 1}^P \beta_p X_p + \varepsilon
  \end{align*}
  This type of model is known as the \emph{general linear model}.
  \vc
  \begin{itemize}
  \item All flavors of linear regression are general linear models.
    \vc
    \begin{itemize}
    \item SLR, MLR
      \vc
    \item t-test, ANOVA, ANCOVA
      \vc
    \item Multilevel linear regression models
    \end{itemize}
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Components of the General Linear Model}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}

      We can break our model into pieces:
      \begin{align*}
        \eta &= \beta_0 + \sum_{p = 1}^P \beta_p X_p\\
        Y &= \eta + \varepsilon
      \end{align*}
      $\varepsilon \sim \mathrm{N}(0, \sigma^2)$, so we can also write:
      \begin{align*}
        Y \sim \mathrm{N}(\eta, \sigma^2)
      \end{align*}
      Where:

    \end{column}
    \begin{column}{0.5\textwidth}
      
      \begin{figure}
        \includegraphics[width = \textwidth]{%
          figures/conditional_density_figure.png%
        }
      \end{figure}

    \end{column}
  \end{columns}

  \begin{itemize}
    \item $\eta$ is the \emph{systematic component} of the model (AKA, the
      \emph{linear predictor}).
    \item The normal distribution, $\mathrm{N}(\cdot, \cdot)$, is the model's
      \emph{random component}.
  \end{itemize}

  \vfill

  \tiny{Image retrieved from:
  \url{http://www.seaturtle.org/mtn/archives/mtn122/mtn122p1.shtml}
  }

  \vx{-24}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Components of the General Linear Model}

  The purpose of general linear modeling (i.e., regression modeling) is to build
  a model of the outcome's mean, $\mu_Y$.
  \begin{itemize}
  \item In this case, $\mu_Y = \eta$.
  \item The systematic component defines the mean of $Y$.
  \end{itemize}
  
  \vb

  The random component quantifies variability around $\mu_Y$ (i.e., error
  variance).
  \begin{itemize}
  \item In the general linear model, we assume that this error variance follows
    a normal distribution.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\sectionslide{Generalized Linear Model}

%------------------------------------------------------------------------------%

\begin{frame}{Extending the General Linear Model}

  We can generalize the models we've been using in two important ways:
  \vc
  \begin{enumerate}
  \item Allow for random components other than the normal distribution.
    \vc
  \item Allow for more complicated relations between $\mu_Y$ and $\eta$.
    \begin{itemize}
    \item Allow: $g(\mu_Y) = \eta$
    \end{itemize}
  \end{enumerate}
  \vc
  These extensions lead to the class of \emph{generalized linear models} (GLMs).

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Components of the Generalized Linear Model}

  The random component in a GLM can be any distribution from the so-called
  \emph{exponential family}.
  \vc
  \begin{itemize}
  \item The exponential family contains many popular distributions:
    \vc
    \begin{itemize}
    \item Normal
    \item Binomial
    \item Poisson
    \item Many others...
    \end{itemize}
  \end{itemize}
  \vc
  The systematic component of a GLM is exactly the same as it is in general
  linear models:
  \begin{align*}
    \eta = \beta_0 + \sum_{p = 1}^P \beta_p X_p
  \end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Link Functions}

  In GLMs, $\eta$ does not directly describe $\mu_Y$.
  \begin{itemize}
    \item We first transform $\mu_Y$ via a \emph{link function}.
    \item $g(\mu_Y) = \eta$
  \end{itemize}
  \vb
  The link function performs two important functions.
  \vc
  \begin{enumerate}
    \item Linearize the association between $\mathbf{X}$ and $Y$.
      \begin{itemize}
        \item Nonlinear: $\mathbf{X} \rightarrow \mu_Y$
        \item Linear: $\mathbf{X} \rightarrow g(\mu_Y)$
      \end{itemize}
      \vc
    \item Allows GLMs for outcomes with restricted ranges without
      requiring any restrictions on the range of the $\{X_p\}$.
      \begin{itemize}
        \item In many cases, $\mu_Y$ has a limited range.
          \begin{itemize}
            \item Counts: $\mu_Y > 0$
            \item Probabilities: $\mu_Y \in [0, 1]$
          \end{itemize}
        \item When correctly specified, $g(\mu_Y)$ can take any value on the
          real line.
      \end{itemize}
  \end{enumerate}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Visualizing Link Functions}

  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
x <- seq(-2, 3, length.out = 10000)
y <- 3 * exp(x)

dat2 <- data.frame(X = x, y)

ggplot(dat2, aes(X, y)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  theme_minimal() +
  scale_x_continuous(expand = c(0, 0)) +
  ylab(expression(mu))  +
  ggtitle("Raw Conditional Mean\n") +
  theme(axis.title = element_text(family = "serif", face = "italic", size = 14),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16)
       )
@

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
ggplot(dat2, aes(X, log(y))) +
  geom_line(color = "blue", linewidth = 1) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  theme_minimal() +
  scale_x_continuous(expand = c(0, 0)) +
  ylab(expression(paste("g(", mu, ")"))) +
  ggtitle("Linearized Conditional Mean\n") +
  theme(axis.title = element_text(family = "serif", face = "italic", size = 14),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16)
       )
@

    \end{column}
  \end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Visualizing Link Functions}

  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
x <- 3 + seq(-5, 5, length.out = 10000)
y <- plogis(x - 3)

dat2 <- data.frame(X = x, y)

ggplot(dat2, aes(X, y)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  theme_minimal() +
  scale_x_continuous(expand = c(0, 0)) +
  ylab(expression(mu))  +
  ggtitle("Raw Conditional Mean\n") +
  theme(axis.title = element_text(family = "serif", face = "italic", size = 14),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16)
       )
@

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
ggplot(dat2, aes(X, qlogis(y))) +
  geom_line(color = "blue", linewidth = 1) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  theme_minimal() +
  scale_x_continuous(expand = c(0, 0)) +
  ylab(expression(paste("g(", mu, ")"))) +
  ggtitle("Linearized Conditional Mean\n") +
  theme(axis.title = element_text(family = "serif", face = "italic", size = 14),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16)
       )
@

    \end{column}
  \end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Components of the Generalized Linear Model}

  Every GLM is built from three components:
  \vb
  \begin{enumerate}
    \item The systematic component, $\eta$.
      \begin{itemize}
        \item A linear function of the predictors, $\{X_p\}$.
        \item Describes the association between $\mathbf{X}$ and $Y$.
      \end{itemize}
      \vb
    \item The link function, $g(\mu_Y)$.
      \begin{itemize}
        \item Linearizes the relation between $\mathbf{X}$ and $Y$.
        \item Transforms $\mu_Y$ so that it can take any value on the real line.
      \end{itemize}
      \vb
    \item The random component, $P(Y|g^{-1}(\eta))$
      \begin{itemize}
        \item The distribution of the observed $Y$.
        \item Quantifies the error variance around $\eta$.
      \end{itemize}
  \end{enumerate}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{General Linear Model as a Special Case}

  The general linear model is a special case of GLM.
  \vb
  \begin{enumerate}
  \item Systematic component:
    \begin{align*}
      \eta = \beta_0 + \sum_{p = 1}^P \beta_p X_p
    \end{align*}
  \item Link function:
    \begin{align*}
      \mu_Y = \eta
    \end{align*}
  \item Random component:
    \begin{align*}
      Y \sim \mathrm{N}(\eta, \sigma^2)
    \end{align*}
  \end{enumerate}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

  <<>>=
  data(iris)

  ## General linear model:
  lmFit <- lm(Petal.Length ~ Petal.Width + Species, data = iris)

  ## Generalized linear model:
  glmFit <- glm(Petal.Length ~ Petal.Width + Species,
                family = gaussian(link = "identity"),
                data = iris)
  @

  \pagebreak

  <<>>=
  partSummary(lmFit, 3)
  partSummary(glmFit, 2)
  @

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\sectionslide{Logistic Regression}

%------------------------------------------------------------------------------%

\begin{frame}{Logistic Regression}

  So why do we care about the GLM when linear regression models have worked
  thus far?
  \begin{itemize}
  \item In a word: Classification.
  \end{itemize}
  \vb
  In the classification task, we have a discrete, qualitative outcome.
  \begin{itemize}
  \item We will begin with the situation of two-level outcomes.
    \begin{itemize}
    \item Alive or Dead
    \item Pass or Fail
    \item Pay or Default
    \end{itemize}
  \end{itemize}
  \vb
  We want to build a model that predicts class membership based on some set of
  interesting features.
  \begin{itemize}
  \item To do so, we will use a very useful type of GLM: \emph{logistic
    regression}.
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Classification Example}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      Suppose we want to know the effect of study time on the probability of
      passing an exam.
      \vc
      \begin{itemize}
      \item The probability of passing must be between 0 and 1.
        \vc
      \item We care about the probability of passing, but we only observe
        absolute success or failure.
        \vc
        \begin{itemize}
        \item $Y \in \{1, 0\}$
        \end{itemize}
      \end{itemize}

      \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
beta <- c(-5.5, 1)
x    <- runif(100, 0, 12)
eta  <- beta[1] + beta[2] * x
pi   <- exp(eta) / (1 + exp(eta))
y    <- rbinom(100, 1, pi)

dat1 <- data.frame(x, y)

p1 <- gg0(x = dat1$x, y = dat1$y) +
    xlab("Hours of Study") +
    ylab("Probability of Passing")
p1
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Linear Regression for Binary Outcomes?}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      What happens if we try to model these data with linear regression?
      \vc
      \begin{itemize}
      \item Hmm...notice any problems?
      \end{itemize}

    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p1 + geom_smooth(method = "lm", se = FALSE)
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Logistic Regression Visualized}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      We get a much better model using logistic regression.
      \vc
      \begin{itemize}
      \item The link function ensures legal predicted values.
        \vc
      \item The sigmoidal curve implies fluctuation in the effectiveness of
        extra study time.
        \vc
        \begin{itemize}
        \item More study time is most beneficial for students with around
          \Sexpr{round(-beta[1] / beta[2], 2)} hours of study.
        \end{itemize}
      \end{itemize}

    \end{column}

    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
tmp  <- with(dat1, seq(min(x), max(x), length.out = 500))
eta2 <- beta[1] + beta[2] * tmp
pi2  <- exp(eta2) / (1 + exp(eta2))
dat2 <- data.frame(x = tmp, y = pi2)

p1 + geom_line(aes(x = x, y = y), data = dat2, colour = "blue", linewidth = 1)
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Probabilities, Odds, \& Odds-Ratios}

<<echo = FALSE>>=
utmb <- readRDS("../data/utmb_finish_2017.rds")
@

In 2017, \Sexpr{nrow(utmb)} people participated in the \emph{Ultra-Trail du
  Mont-Blanc}, but only \Sexpr{round(100 * mean(utmb$Finish == "Yes"), 2)}\%
  finished the race.
\vc
\begin{itemize}
\item Below, you can find a cross-tabulation of finishing status and sex.
\end{itemize}

<<echo = FALSE, results = "asis">>=
tab <- xtabs(~ Sex + Finish, data = utmb)

xTab <- xtable(tab, digits = 0)

adds <- list(pos = list(0, 0),
             command = c("& \\multicolumn{2}{c}{Finish} \\\\\n",
                         "Sex & No & Yes \\\\\n")
             )

print(xTab, add.to.row = adds, include.colnames = FALSE)
@
\vb
\begin{itemize}
\item What is the \emph{probability} of finishing for each sex?
\vc
\item What are the \emph{odds} of finishing for each sex?
\vc
\item What is the \emph{odds ratio} of finishing for males vs. females?
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Defining the Logistic Regression Model}

  In logistic regression problems, we are modeling binary data:
  \begin{itemize}
  \item Usual coding: $Y \in \{1 = \text{``Success''}, 0 = \text{``Failure''} \}$.
  \end{itemize}
  \vb
  The \emph{Binomial} distribution is a good way to represent this kind of data.
  \begin{itemize}
  \item The systematic component in our logistic regression model will be the
    binomial distribution.
  \end{itemize}
  \vb
  The mean of the binomial distribution (with $N = 1$) is the ``success''
  probability, $\pi = P(Y = 1)$.
  \begin{itemize}
  \item We are interested in modeling $\mu_Y = \pi$:
    \begin{align*}
      g(\pi) = \beta_0 + \sum_{p = 1}^P \beta_p X_p
    \end{align*}
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Link Function for Logistic Regression}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      Because $\pi$ is bounded by 0 and 1 and not linear related to $\mathbf{X}$, 
      we cannot model it directly---we must apply an appropriate link function.
      \vc
      \begin{itemize}
        \item Logistic regression uses the \emph{logit link}.
      \end{itemize}

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
library(ggplot2)

x <- 3 + seq(-5, 5, length.out = 10000)
p <- plogis(x - 3)
o <- p / (1 - p)

dat2 <- data.frame(X = x, p, o)

ggplot(dat2, aes(X, p)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  theme_minimal() +
  scale_x_continuous(expand = c(0, 0)) +
  ylab(expression(pi))  +
  ggtitle("Probability Model\n") +
  theme(axis.title = element_text(family = "serif", face = "italic", size = 14),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16)
       )
@

    \end{column}
  \end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Link Function for Logistic Regression}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      Given $\pi$, we can define the \emph{odds} of success as:
      \begin{align*}
        O_s = \frac{\pi}{1 - \pi}
      \end{align*}
      \begin{itemize}
        \item Because $\pi \in [0, 1]$, we know that $O_s \geq 0$.
      \end{itemize}

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
ggplot(dat2, aes(X, o)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  theme_minimal() +
  scale_x_continuous(expand = c(0, 0)) +
  ylab("O(Y = 1)")  +
  ggtitle("Odds Model\n") +
  theme(axis.title = element_text(family = "serif", face = "italic", size = 14),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16)
       )
@

    \end{column}
  \end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Link Function for Logistic Regression}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      We take the natural log of the odds as the last step to fully map $\pi$ to
      the real line.
      \begin{align*}
        \mathrm{logit}(\pi) = \ln \left(\frac{\pi}{1 - \pi}\right)
      \end{align*}

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
ggplot(dat2, aes(X, qlogis(p))) +
  geom_line(color = "blue", linewidth = 1) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  theme_minimal() +
  scale_x_continuous(expand = c(0, 0)) +
  ylab(expression(paste("logit(", pi, ")"))) +
  ggtitle("Logit Model\n") +
  theme(axis.title = element_text(family = "serif", face = "italic", size = 14),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16)
       )
@

    \end{column}
  \end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Fully Specified Logistic Regression Model}

  Our final logistic regression model is:
  \begin{align*}
    Y &\sim \mathrm{Bin}(\pi, 1)\\
    \mathrm{logit}(\pi) &= \beta_0 + \sum_{p = 1}^P \beta_p X_p
  \end{align*}
  The fitted model can be represented as:
  \begin{align*}
    \mathrm{logit}(\hat{\pi}) = \hat{\beta}_0 + \sum_{p = 1}^P \hat{\beta}_p X_p
  \end{align*}
  The fitted coefficients, $\{\hat{\beta}_0, \hat{\beta}_p\}$, are interpreted
  in units of \emph{log odds}.

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Logistic Regression Example}

<<echo = FALSE>>=
out1 <- glm(y ~ x, data = dat1, family = binomial())

b0 <- round(coef(out1)[1], 3)
b1 <- round(coef(out1)[2], 3)

b0e <- round(exp(coef(out1))[1], 3)
b1e <- round(exp(coef(out1))[2], 3)
@

If we fit a logistic regression model to the test-passing data plotted above, we
get:
\begin{align*}
  \mathrm{logit}(\hat{\pi}_{pass}) = \Sexpr{b0} + \Sexpr{b1} X_{study}
\end{align*}
\vx{-12}
\begin{itemize}
\item A student who does not study at all has \Sexpr{b0} log odds of passing the
  exam.
\item For each additional hour of study, a student's log odds of passing increase
  by \Sexpr{b1} units.
\end{itemize}
\vb
Log odds do not lend themselves to interpretation.
\begin{itemize}
\item We can convert the effects back to an odds scale by exponentiation.
\item $\hat{\beta}$ has log odds units, but $e^{\hat{\beta}}$ has odds units.
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Interpretations}

  Exponentiating the coefficients also converts the additive effects to
  multiplicative effects.
  \vc
  \begin{itemize}
  \item We can interpret $\hat{\beta}$ as we would in linear regression:
    \begin{itemize}
    \item A unit change in $X_p$ produces an expected change of $\hat{\beta}_p$
      units in $\mathrm{logit}(\pi)$.
    \end{itemize}
    \vc
  \item After exponentiation, however, unit changes in $X_p$ imply multiplicative
    changes in $O_s = \pi / (1 - \pi)$.
    \begin{itemize}
    \item A unit change in $X_p$ results in multiplying $O_s$  by
      $e^{\hat{\beta}_p}$.
    \end{itemize}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Interpretations}

  Exponentiating the coefficients in our toy test-passing example produces the
  following interpretations:
  \begin{itemize}
  \item A student who does not study is expected to pass the exam with odds of
    \Sexpr{b0e}.
  \item For each additional hour a student studies, their odds of passing
    increase by \Sexpr{b1e} \emph{times}.
    \begin{itemize}
    \item Odds of passing are \emph{multiplied} by \Sexpr{b1e} for each extra
      hour of study.
    \end{itemize}
  \end{itemize}

  \vb
  \pause

  Due to the confusing interpretations of the coefficients, we often focus
  on the valance of the effects:
  \begin{itemize}
  \item Additional study time is associated with increased odds of passing.
  \item $\hat{\beta_p} > 0$ = ``Increased Success'',  $e^{\hat{\beta}_p} > 1$ =
    ``Increased Success''
  \end{itemize}
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Visualizing Interpretations}

  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
dat1$p <- predict(out1, type = "response")
dat1$o <- with(dat1, p / (1 - p))

ggplot(dat1, aes(x, qlogis(p))) +
  geom_line(color = "blue", linewidth = 1) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  theme_minimal() +
  scale_x_continuous(expand = c(0, 0)) +
  ylab("Log-Odds of Passing")  +
  xlab("Study Hours") +
  ggtitle("Logit Model\n") +
  theme(text = element_text(family = "Courier", size = 16),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16)
  )
@

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
ggplot(dat1, aes(x, o)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  theme_minimal() +
  scale_x_continuous(expand = c(0, 0)) +
  ylab("Odds of Passing")  +
  xlab("Study Hours") +
  ggtitle("Odds Model\n") +
  theme(text = element_text(family = "Courier", size = 16),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16)
  )
@

    \end{column}
  \end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Visualizing Interpretations}

  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
ggplot(dat1, aes(x, o)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  theme_minimal() +
  scale_x_continuous(expand = c(0, 0)) +
  ylab("Odds of Passing")  +
  xlab("Study Hours") +
  ggtitle("Odds Model\n") +
  theme(text = element_text(family = "Courier", size = 16),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16)
  )
@

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
ggplot(dat1, aes(x, p)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  theme_minimal() +
  scale_x_continuous(expand = c(0, 0)) +
  ylab("Probability of Passing")  +
  xlab("Study Hours") +
  ggtitle("Probability Model\n") +
  theme(text = element_text(family = "Courier", size = 16),
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16)
  )
@

    \end{column}
  \end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

  Let's use logistic regression to compute the odds of finishing the UTMB.
  
  <<eval = FALSE>>=
  ## Read the UTMB data:
  utmb <- readRDS(paste0(dataDir, "utmb_finish_2017.rds"))
  @
  
  We use the \src{glm()} function to estimate generalized linear models.
  \vc
  \begin{itemize}
  \item To get a logistic regression model, we need to do two things:
    \vc
    \begin{enumerate}
    \item Specify a binary outcome variable
      \vc
    \item Specify the \src{family = 'binomial'} argument.
    \end{enumerate}
  \end{itemize}
  
  <<>>=
  ## Estimate the logistic regression model:
  fit <- glm(Finish ~ Sex, family = binomial(link = "logit"), data = utmb)
  @

  \pagebreak

  <<>>=
  partSummary(fit, -1)
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Interpretation}

<<echo = FALSE, include = FALSE>>=
results <- getRegStats(fit, fit = FALSE)
@

  \rmsc{Intercept}

  \vb

  The expected log-odds of a female finishing the race are \Sexpr{results$b[1]}.

  \va

  \rmsc{Slope}

  \vb

  A runner's sex significantly predicts their probability of finishing the race (%
    $\beta = \Sexpr{results$b[2]}$,
    $z = \Sexpr{results$t[2]}$,
    $\Sexpr{results$p[2]}$%
  ).
  \vc
  \begin{itemize}
    \item The expected log-odds of males finishing the race are \Sexpr{results$b[2]}
      units higher than expected log-odds of females finishing the race.
  \end{itemize}


\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Interpretation}

  The raw coefficient estimates are in units of log-odds.
  \begin{itemize}
  \item We need to exponentiate the estimates to get odds ratios.
  \end{itemize}
  
  <<>>=
  library(dplyr)
  
  coef(fit) %>% exp()
  @

  <<echo = FALSE, include = FALSE>>=
  oHat <- coef(fit) |> exp() |> round(2)
  @

  \rmsc{Intercept}

  \vc

  The expected odds of a female finishing the race are \Sexpr{oHat[1]}.

  \vb

  \rmsc{Slope}

  \vc

  The expected odds of males finishing the race are \Sexpr{oHat[2]} times the
  expected odds of females finishing the race.

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Multiple Logistic Regression}

  The preceding example was a \emph{simple logistic regression}.
  \vc
  \begin{itemize}
  \item Including multiple predictor variables in the systematic component leads
    to \emph{multiple logistic regression}.
    \vc
  \item The relative differences between simple logistic regression and multiple
    logistic regression are the same as those between simple linear regression
    and multiple linear regression.
    \vc
    \begin{itemize}
    \item The only important complication is that the regression coefficients
      become partial effects.
    \end{itemize}
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

  Let's use logistic regression to predict the chances that Titanic passengers
  survived the sinking based on their age, sex, and ticket class.
  
  <<>>=
  ## Read the data:
  titanic <- readRDS(paste0(dataDir, "titanic.rds"))

  ## Estimate the logistic regression model:
  fit <- glm(survived ~ age + sex + class,
             data = titanic,
             family = "binomial")
  @

  \pagebreak

  <<>>=
  partSummary(fit, -1)
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Interpretation}

<<echo = FALSE, include = FALSE>>=
results <- getRegStats(fit, fit = FALSE)
@

  \rmsc{Intercept}

  \vb

  The expected log-odds of survival for a zero-year-old female passenger in
  first class are \Sexpr{results$b[1]}.

  \va

  \rmsc{Age Effect}

  \vb

  A passenger's age significantly predicts their probability of survival, after
  controlling for their gender and ticket class (%
    $\beta = \Sexpr{results$b[2]}$,
    $z = \Sexpr{results$t[2]}$,
    $\Sexpr{results$p[2]}$%
  ).
  \vc
  \begin{itemize}
    \item For each additional year of age, the expected log-odds of survival
      decrease by \Sexpr{abs(results$b[2])} units, after controlling for gender
      and ticket class.
  \end{itemize}

  \pagebreak

  \rmsc{Gender Effect}

  \vb

  A passenger's gender significantly predicts their probability of survival, after
  controlling for their age and ticket class (%
    $\beta = \Sexpr{results$b[3]}$,
    $z = \Sexpr{results$t[3]}$,
    $\Sexpr{results$p[3]}$%
  ).
  \vc
  \begin{itemize}
    \item The expected log-odds of survival are \Sexpr{abs(results$b[3])} units
      lower for men/boys than for women/girls, after controlling for age and ticket class.
  \end{itemize}

  \pagebreak

  \rmsc{Class Effect}

  \vb

  After controlling for age and gender, there is a significant difference in
  predicted survival probability between passengers in second class and
  passengers in first class (%
    $\beta = \Sexpr{results$b[4]}$,
    $z = \Sexpr{results$t[4]}$,
    $\Sexpr{results$p[4]}$%
  ) and between passengers in third class and passengers in first class (%
    $\beta = \Sexpr{results$b[5]}$,
    $z = \Sexpr{results$t[5]}$,
    $\Sexpr{results$p[5]}$%
  ).
  \vc
  \begin{itemize}
    \item The expected log-odds of survival are \Sexpr{abs(results$b[4])} units
      lower for passengers in second class than for passengers in first class,
      after controlling for age and gender.
      \vc
    \item The expected log-odds of survival are \Sexpr{abs(results$b[5])} units
      lower for passengers in third class than for passengers in first class,
      after controlling for age and gender.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

  Compute odds ratios.
  
  <<>>=
  (or <- coef(fit) %>% exp())
  @

  Odds ratios smaller than 1.0 can be difficult to explain.
  \begin{itemize}
  \item We can ease interpretation by reciprocating the estimates.
  \end{itemize}
  
  <<>>=
  1 / or
  @

  \pagebreak

  To convince ourselves that the above operation is sensible, we can compare the
  inverse odds ratios to the odds ratios we get from predicting the chances of
  dying.

  <<>>=
  library(magrittr)
  
  fit2 <- titanic %>%
      mutate(died = relevel(survived, ref = "yes")) %$% 
      glm(died ~ age + sex + class, family = "binomial")
  @

  \pagebreak

  <<>>=
  partSummary(fit2, -1)
  @

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

  We get the same odds ratios that we derived through reciprocation.
  
  <<>>=
  coef(fit2) %>% exp()
  1 / or
  @

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Interpretation}

<<echo = FALSE, include = FALSE>>=
or1 <- round(or, 2)
or2 <- round(1 / or, 2)
@

  \rmsc{Intercept}

  \vb

  \begin{itemize}
    \item The expected odds of survival for a zero-year-old female passenger in
      first class are \Sexpr{or1[1]}.

      \vc

    \item The expected odds of death for a zero-year-old female passenger in
      first class are \Sexpr{or2[1]}.

  \end{itemize}

  \pagebreak

  \rmsc{Age Effect}

  \vb

  \begin{itemize}
    \item For each additional year of age, the expected odds of survival change by a
      factor of \Sexpr{or1[2]} times, after controlling for gender and ticket class.

      \vc

    \item For any two passengers with a one year age difference, the expected odds of
      the older passenger surviving are \Sexpr{or1[2]} times the expected odds of
      the younger passenger surviving, after controlling for their genders and
      ticket classes.

      \vc

    \item For each additional year of age, the expected odds of death are \Sexpr{or2[2]}
      times higher, after controlling for gender and ticket class.

  \end{itemize}

  \pagebreak

  \rmsc{Gender Effect}

  \vb

  \begin{itemize}
    \item The expected odds of survival for men/boys are \Sexpr{or1[3]} times the expected
      odds of survival for women/girls, after controlling for age and ticket class.

      \vc

    \item The expected odds of death are \Sexpr{or2[3]} times higher for men/boys than
      for women/girls, after controlling for age and ticket class.

  \end{itemize}

  \pagebreak

  \rmsc{Class Effect}

  \vb

  \begin{itemize}
    \item The expected odds of survival for passengers in second class are \Sexpr{or1[4]}
      times the expected odds of survival for passengers in first class, after
      controlling for gender and age.

      \vc

    \item The expected odds of death are \Sexpr{or2[4]} times higher for passengers in
      second class than for passengers in first class, after controlling for gender and age.

      \vc

    \item The expected odds of survival for passengers in third class are \Sexpr{or1[5]}
      times the expected odds of survival for passengers in first class, after
      controlling for gender and age.

      \vc

    \item The expected odds of death are \Sexpr{or2[5]} times higher for passengers in
      third class than for passengers in first class, after controlling for gender and age.

  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Example in Equations}

  <<echo = FALSE>>=
  beta1  <- coef(fit2)
  eBeta1 <- exp(beta1)
  
  b0 <- round(beta1[1], 2)
  b1 <- round(beta1[2], 2)
  b2 <- round(beta1[3], 2)
  b3 <- round(beta1[4], 2)
  b4 <- round(beta1[5], 2)
  
  eB0 <- round(eBeta1[1], 2)
  eB1 <- round(eBeta1[2], 2)
  eB2 <- round(eBeta1[3], 2)
  eB3 <- round(eBeta1[4], 2)
  eB4 <- round(eBeta1[5], 2)
  @
  
  Here's the symbolic representation of our logistic regression model:
  \begin{align*}
    \mathrm{logit}(\pi_{died}) =
    \beta_0 +
    \beta_1 X_{age} +
    \beta_2 X_{male} +
    \beta_3 X_{2nd} +
    \beta_4 X_{3rd}
  \end{align*}
  \va
  By fitting this model to the \emph{titanic} data we get:
  \begin{align*}
    \mathrm{logit}(\hat{\pi}_{died}) =
    \Sexpr{b0} +
    \Sexpr{b1} X_{age} + 
    \Sexpr{b2} X_{male} +
    \Sexpr{b3} X_{2nd} +
    \Sexpr{b4} X_{3rd}
  \end{align*}
  \va
  Exponentiating the coefficients produces:
  \begin{align*}
    \frac{\hat{\pi}_{died}}{1 - \hat{\pi}_{died}} =
    \frac{\hat{\pi}_{died}}{\hat{\pi}_{survived}} 
    = \Sexpr{eB0} \times
    \Sexpr{eB1}^{X_{age}} \times
    \Sexpr{eB2}^{X_{male}} \times
    \Sexpr{eB3}^{X_{2nd}} \times
    \Sexpr{eB4}^{X_{3rd}}
  \end{align*}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Exponentiating the Systematic Component}
  
  \begin{align*}
    \mathrm{logit}(\hat{\pi}_{died}) &=
    \Sexpr{b0} +
    \Sexpr{b1} X_{age} +
    \Sexpr{b2} X_{male} +
    \Sexpr{b3} X_{2nd} +
    \Sexpr{b4} X_{3rd}\\[12pt]
    e^{\mathrm{logit}(\hat{\pi}_{died})} &=
    e^{\left(
    \Sexpr{b0} ~+~
    \Sexpr{b1} X_{age} ~+~
    \Sexpr{b2} X_{male} ~+~
    \Sexpr{b3} X_{2nd} ~+~
    \Sexpr{b4} X_{3rd}
    \right)}\\[8pt]
    \frac{\hat{\pi}_{died}}{\hat{\pi}_{survived}} &=
    e^{\Sexpr{b0}} \times
    e^{\Sexpr{b1} X_{age}} \times
    e^{\Sexpr{b2} X_{male}} \times
    e^{\Sexpr{b3} X_{2nd}} \times
    e^{\Sexpr{b4} X_{3rd}}\\[6pt]
    &= e^{\Sexpr{b0}} \times
    \left(e^{\Sexpr{b1}}\right)^{X_{age}} \times
    \left(e^{\Sexpr{b2}}\right)^{X_{male}} \times
    \left(e^{\Sexpr{b3}}\right)^{X_{2nd}} \times
    \left(e^{\Sexpr{b4}}\right)^{X_{3rd}}\\[12pt]
    &= \Sexpr{eB0} \times
    \Sexpr{eB1}^{X_{age}} \times
    \Sexpr{eB2}^{X_{male}} \times
    \Sexpr{eB3}^{X_{2nd}} \times
    \Sexpr{eB4}^{X_{3rd}}
  \end{align*}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Model Comparison}

  <<>>=
  ## Estimate a restricted model:
  fit0 <- update(fit, ". ~ . - class")

  ## Check the result:
  partSummary(fit0, 1:3)
  @

  \pagebreak

  We don't have an $R^2$ statistic for logistic regression models, so we need to
  use a \emph{likelihood ratio test} to compare nested models.
  
  <<>>=
  anova(fit0, fit, test = "LRT")
  @

  \pagebreak

  We can also use information criteria.
  
  <<>>=
  AIC(fit0, fit)
  BIC(fit0, fit)
  @
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\sectionslide{Classification}

%------------------------------------------------------------------------------%

\begin{frame}{Predictions from Logistic Regression}

  Given a fitted logistic regression model, we can get predictions for new
  observations of $\{X_p\}$, $\{X_p'\}$.
  \vc
  \begin{itemize}
  \item Directly applying $\{\hat{\beta}_0, \hat{\beta}_p\}$ to $\{X_p'\}$ will
    produce predictions on the scale of $\eta$:
    \begin{align*}
      \hat{\eta}' = \hat{\beta}_0 + \sum_{p = 1}^P \hat{\beta}_p X_p'
    \end{align*}
  \item By applying the inverse link function, $g^{-1}(\cdot)$, to
    $\hat{\eta}'$, we get predicted success probabilities:
    \begin{align*}
      \hat{\pi}' = g^{-1}(\hat{\eta}')
    \end{align*}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Predictions from Logistic Regression}

  In logistic regression, the inverse link function, $g^{-1}(\cdot)$, is the
  \emph{logistic function}:
  \begin{align*}
    \mathrm{logistic}(X) = \frac{e^X}{1 + e^X}
  \end{align*}
  So, we convert $\hat{\eta}'$ to $\hat{\pi}'$ by:
  \begin{align*}
    \hat{\pi}' &= \frac{e^{\hat{\eta}'}}{1 + e^{\hat{\eta}'}} = \frac{\exp \left(
      \hat{\beta}_0 + \sum_{p = 1}^P \hat{\beta}_p X_p' \right) }{1 + \exp \left(
      \hat{\beta}_0 + \sum_{p = 1}^P \hat{\beta}_p X_p' \right) }
  \end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Classification with Logistic Regression}

  Once we have computed the predicted success probabilities, $\hat{\pi}'$, we
  can use them to classify new observations.
  \vc
  \begin{itemize}
  \item By choosing a threshold on $\hat{\pi}'$, say $\hat{\pi}' = t$, we can
    classify the new observations as ``Successes'' or ``Failures'':
    \begin{align*}
      \hat{Y}' = \left\{
      \begin{array}{ccc}
        1 & if & \hat{\pi}' \geq t\\
        0 & if & \hat{\pi}' < t\\
      \end{array}
      \right.
    \end{align*}

  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Classification Example}
  
  <<echo = FALSE>>=
  xn   <- c(1, 17, 1, 1, 0)
  eta1 <- crossprod(xn, beta1)
  pi1  <- round(exp(eta1) / (1 + exp(eta1)), 3)
  eta1 <- round(eta1, 3)
  g    <- as.numeric(pi1 > 0.5) + 1
  @
  
  Say we want to classify a hypothetical passenger as either having died or
  survived the sinking.
  \begin{itemize}
  \item Assume this passenger has the following characteristics:
    \begin{itemize}
    \item They are 17 years old
    \item They are male
    \item They are a second class passenger
    \end{itemize}
  \end{itemize}
  \vb
  First we plug their predictor data into the fitted model to get their
  model-implied $\eta$:
  \begin{align*}
    \hat{\eta}_{died} &=
                        \Sexpr{b0} +
                        \Sexpr{b1} \times 17 +
                        \Sexpr{b2} \times 1 +
                        \Sexpr{b3} \times 1 +
                        \Sexpr{b4} \times 0\\[6pt]
                      &= \Sexpr{eta1}
  \end{align*}
  
  \pagebreak
  
  Next we convert the predicted $\eta$ value into a model-implied success
  probability by applying the logistic function:
  \begin{align*}
    \frac{e^{\Sexpr{eta1}}}{1 + e^{\Sexpr{eta1}}} = \Sexpr{pi1}
  \end{align*}\\
  \vb
  Finally, to make the classification, assume a threshold of $\hat{\pi}' = 0.5$
  as the decision boundary.
  \begin{itemize}
  \item Because $\Sexpr{pi1} \Sexpr{c("<", ">")[g]} 0.5$ we would classify this
    passenger as having \Sexpr{c("survived", "died")[g]}.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\end{document}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

  Let's generate a grid of hypothetical passengers to explore the model's
  predictions.
  
  <<>>=
  passengers <-
      expand.grid(
          age   = 1:100,
          sex   = c("male", "female"),
          class = c("1st", "2nd", "3rd")
      ) %>%
      data.frame()
  @

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

  View 10 random passengers.
  
  <<>>=
  passengers %>% slice_sample(n = 10)
  @ 

  \pagebreak

  We can generate predictions on the scale of the linear predictor (i.e.,
  log-odds), or we can generate predicted probabilities.
  
  <<>>=
  passengers %<>%
      mutate( 
          ## Predicted log odds of dying:
          etaHat = predict(fit2, newdata = ., type = "link"),

          ## Predicted probabilities of dying:
          piHat = predict(fit2, newdata = ., type = "response")
      )
  @

  We can then use the predicted probabilities of dying to classify:

  <<>>=
  passengers %<>%
      mutate(dieHat = ifelse(piHat > 0.5, "dead", "alive") %>% factor())
  @

  \pagebreak

  View the predictions for 10 random passengers:
  
  <<>>=
  passengers %>% slice_sample(n = 10)
  @

  \pagebreak

  We can visualize the predicted probabilities of dying.
  
  <<eval = FALSE>>=
  library(ggplot2)
  
  ggplot(passengers, aes(age, piHat, color = sex)) +
      geom_line() +
      facet_wrap(vars(class)) +
      ylab("Predicted Probability of Death")
  @ 

  \pagebreak

  <<echo = FALSE, out.width = "65%">>=
  library(ggplot2)
  
  ggplot(passengers, aes(age, piHat, color = sex)) +
      geom_line() +
      facet_wrap(vars(class)) +
      ylab("Predicted Probability of Death")
  @ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

  We can also visualize the classifications the model would make.
  
  <<eval = FALSE>>=
  ggplot(passengers, aes(dieHat, age, color = class)) +
      geom_boxplot() +
      facet_wrap(vars(sex)) +
      xlab("Predicted Death")
  @ 

  \pagebreak

  <<echo = FALSE, out.width = "65%">>=
  ggplot(passengers, aes(dieHat, age, color = class)) +
      geom_boxplot() +
      facet_wrap(vars(sex)) +
      xlab("Predicted Death")
  @ 

\end{frame}

%------------------------------------------------------------------------------%

\end{document}

