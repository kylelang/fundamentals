%%% Title:    FTDS Lecture 7: Logistic regression
%%% Author:   Mingyang Cai
%%% Created:  2022-12-03
%%% Modified: 2022-12-19

\documentclass[10pt]{beamer}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0, 0, 0}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.69,0.494,0}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.749,0.012,0.012}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.514,0.506,0.514}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0,0.341,0.682}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.004,0.004,0.506}{#1}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usetheme{Utrecht}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{listings}
\usepackage{xspace}
\usepackage{tcolorbox}
\tcbuselibrary{listings}
\usepackage{hyperref}
\usepackage{caption}
\captionsetup{labelformat=empty}

\hypersetup{
  colorlinks = false,
  linkcolor = blue,
  filecolor = blue,
  citecolor = black,
  urlcolor = blue
}

\definecolor{codebackground}{RGB}{224,234,238}
\definecolor{codestring}{RGB}{191,3,3}
\definecolor{codekeyword}{RGB}{1,1,129}
\definecolor{codecomment}{RGB}{131,129,131}

\newcommand{\src}[1]{%
  \tcbox[%
  on line,
  colback = codebackground,
  colframe = codebackground,
  left = 0pt,
  right = 0pt,
  top = 0pt,
  bottom = 0pt%
  ]{%
    \lstinline[%
    language = R,
    basicstyle = \ttfamily,
    keywordstyle = \color{codekeyword},
    commentstyle = \color{codecomment}\itshape,
    stringstyle = \color{codestring},
    deletekeywords = {_}
    % frame = single,
    % frameround = tttt,
    % fillcolor = \color{blue}%
    ]{#1}%
  }
}

% \DeclareTotalTCBox{\src}
% { s v }
% {verbatim, colupper = white, colback = black!75!white, colframe = black}
% {%
% \IfBooleanT{#1}{\textcolor{red}{\ttfamily\bfseries >}}%
% \lstinline[language = command.com, keywordstyle = \color{blue!35!white}\bfseries]^#2^%
% }

%   \newtcbinputlisting[]{\src}[1][]{
%   listing only,
%   nobeforeafter,
%   after={\xspace},
%   hbox,
%   tcbox raise base,
%   fontupper=\ttfamily,
%   colback=lightgray,
%   colframe=lightgray,
%   size=fbox
% }{#1}

%   \newcommand{\src}[1]{%
%   \begin{tcbwritetemp} \tcboxverb[beamer]{#1} \end{tcbwritetemp}%
%   \tcbusetemp%
% }
\newcommand{\rmsc}[1]{\textrm{\textsc{#1}}}
\newcommand{\pkg}[1]{\textbf{#1}}
% \newcommand{\src}[1]{\texttt{#1}}

\newcommand{\pipe}{\texttt{\%>\%}}
\newcommand{\expipe}{\texttt{\%\$\%}}
\newcommand{\apipe}{\texttt{\%<>\%}}
\newcommand{\rpipe}{\texttt{|>}}

\title{Logistic regression}
\subtitle{Fundamental Techniques in Data Science}
\author{Mingyang Cai}
\institute{Department of Methodology \& Statistics\\Utrecht University}
\date{}



%------------------------------------------------------------------------------%

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\begin{frame}[t, plain]
  \titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%------------------------------------------------------------------------------%

\subsection{Recap}

%------------------------------------------------------------------------------%

\begin{frame}{Generalized linear models (GLMs)}
\begin{itemize}
\item The mean $E(Y|X) = g^{-1}(\eta) = g^{-1}(\beta_0 + \sum_{p = 1}^P \beta_p X_p)$
\begin{itemize}
\item A linear function $\eta$
\item The link function $g$
\end{itemize}
\item The variance $V(g^{-1}(\eta))$
\end{itemize}
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Logistic regression}
The link function of logistic regression is logit link:
\begin{align*}
ln(\frac{p(Y = 1)}{p(Y=0)}) = \beta_0 + \sum_{p = 1}^P \beta_p X_p
\end{align*}

The interpretation of the coefficients is on log odds units.
\end{frame}

%------------------------------------------------------------------------------%

\subsection{Model assumptions}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Titanic data}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{titanic} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(}\hlkwc{file} \hlstd{=} \hlstr{"data/titanic.csv"}\hlstd{,} \hlkwc{header} \hlstd{=} \hlnum{TRUE}\hlstd{,}
                    \hlkwc{stringsAsFactors} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

The titanic data describes the survival status of passengers on the Titanic. For the heuristic, We only include four variables.
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{titanic} \hlopt{%>%} \hlkwd{head}\hlstd{()}
\end{alltt}
\begin{verbatim}
  Survived Pclass      Age    Sex
1        0      3 22.00000   male
2        1      1 38.00000 female
3        1      3 26.00000 female
4        1      1 35.00000 female
5        0      3 35.00000   male
6        0      3 29.69912   male
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%
  
\begin{frame}[fragile]{Binary response variable} 
\begin{columns}
    \begin{column}{0.5\textwidth}
 
Logistic regression assumes that the response variable only has two possible outcomes.   
\va

For example, ``survived" describes the passenger's survival status where 0 indicates did not survive and 1 indicates survived.

\end{column}

    \begin{column}{0.5\textwidth}

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/r_basics-unnamed-chunk-3-1} 

}


\end{knitrout}
    
    \end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Binary response variable}
We can also check the levels of the response variable.
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{titanic}\hlopt{$}\hlstd{Survived} \hlopt{%>%} \hlkwd{unique}\hlstd{()}
\end{alltt}
\begin{verbatim}
[1] 0 1
\end{verbatim}
\begin{alltt}
\hlstd{titanic}\hlopt{$}\hlstd{Survived} \hlopt{%>%} \hlkwd{factor}\hlstd{()} \hlopt{%>%} \hlkwd{levels}\hlstd{()}
\end{alltt}
\begin{verbatim}
[1] "0" "1"
\end{verbatim}
\end{kframe}
\end{knitrout}

The assumption is violated when the outcome is
\begin{itemize}
\item a multiclass categorical variable. (multinomial logistic regression \src{mnet::mutinom()})
\item an ordinal categorical variable. (ordered logistic regression \src{MASS::polr()})
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Balanced outomes}
The logistic regression may not perform well when there is an imbalance in the classes of the binary response. A possible consequence is the inaccurate classification.
\va

A certain amount of imbalance is normal and can be handled well by the logistic model in most cases. However, we should care about the severe imbalance, for instance, 1000 cases in the majority class and 1 case in the minority class.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{titanic}\hlopt{$}\hlstd{Survived} \hlopt{%>%} \hlkwd{factor}\hlstd{()} \hlopt{%>%} \hlkwd{table}\hlstd{()} \hlopt{%>%}
  \hlkwd{prop.table}\hlstd{()} \hlopt{%>%} \hlkwd{round}\hlstd{(}\hlkwc{digits} \hlstd{=} \hlnum{3}\hlstd{)}
\end{alltt}
\begin{verbatim}
.
    0     1 
0.616 0.384 
\end{verbatim}
\end{kframe}
\end{knitrout}


\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Balanced outomes}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/r_basics-unnamed-chunk-6-1} 

}


\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Balanced outomes}
Some solutions:
\begin{itemize}
  \item down-sampling the majority class
  \item up-sampling the minority class
  \item adding weights to logistic regression (\src{weights} argument in \src{glm()})
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Sufficiently large sample size}
Sample size in logistic regression is a complex issue. It depends on:
\begin{itemize}
 \item the number of predictors
 \item the sample space of predictors
 \item the distribution of the binary response variable
 \item the scientific interests
\end{itemize}
\va
Some suggestions for the sample size
\begin{itemize}
 \item 10 cases for each predictor in the model \citep{agresti2018introduction}
 \item $N = \frac{10*k} {p}$, where
 \begin{itemize}
 \item $k$ is the number of predictors
 \item $p$ is the proportion of the minority class \citep{peduzzi1996simulation}
 \end{itemize}
\item $N = 100 + 50*k$, where $k$ is the number of predictors \citep{bujang2018review}
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Issue : perfect prediction}
Imbalanced outcomes and a small sample size may cause perfect prediction. The \src{glm()} may show warnings messages:
\begin{itemize}
\item glm.fit: algorithm did not converge
\item fitted probabilities numerically 0 or 1 occurred
\end{itemize}
\va
One possible solution is to fit the logistic regression with regularization (\src{glmnet::glmnet}).
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{No multicollinearity}
The same assumption as in linear regression is that is no multicollinearity among the linear predictors.  
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex}\hlopt{*}\hlstd{Pclass,} \hlkwc{family} \hlstd{= binomial,}
    \hlkwc{data} \hlstd{= titanic)} \hlopt{%>%}
   \hlkwd{VIF}\hlstd{()}
\end{alltt}
\begin{verbatim}
       Age        Sex     Pclass Sex:Pclass 
  1.187231  22.400248   7.749858  20.200886 
\end{verbatim}
\end{kframe}
\end{knitrout}

A VIF value larger than 10 indicates high multicollinearity.
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Linearity}
Logistic regression assumes a linear relationship between continuous predictors and \emph{the logit of the response variable}. 
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{pred} \hlkwb{<-} \hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex}\hlopt{*}\hlstd{Pclass,} \hlkwc{family} \hlstd{= binomial,}
            \hlkwc{data} \hlstd{= titanic)} \hlopt{%>%}
        \hlkwd{predict}\hlstd{(.,} \hlkwc{type} \hlstd{=} \hlstr{"link"}\hlstd{)}

\hlkwd{ggplot}\hlstd{(}\hlkwc{data} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{Age} \hlstd{= titanic}\hlopt{$}\hlstd{Age,} \hlkwc{pred} \hlstd{= pred),}
       \hlkwd{aes}\hlstd{(Age, pred))} \hlopt{+}
  \hlkwd{geom_point}\hlstd{(}\hlkwc{size} \hlstd{=} \hlnum{0.5}\hlstd{,} \hlkwc{alpha} \hlstd{=} \hlnum{0.5}\hlstd{)} \hlopt{+}
  \hlkwd{geom_smooth}\hlstd{(}\hlkwc{method} \hlstd{=} \hlstr{"glm"}\hlstd{)} \hlopt{+} \hlkwd{theme_bw}\hlstd{()}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Linearity}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/r_basics-unnamed-chunk-9-1} 

}


\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Linearity}
Some solutions for the violation of linearity:
\begin{itemize}
\item transform predictors (log transformation)
\item add interaction terms or higher-order terms
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{No influential values or outliers}
Influential values or outliers can seriously influence the fit of the logistic regression. 
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex}\hlopt{*}\hlstd{Pclass,} \hlkwc{family} \hlstd{= binomial,}
            \hlkwc{data} \hlstd{= titanic)} \hlopt{%>%} \hlkwd{plot}\hlstd{(.,} \hlkwc{which} \hlstd{=} \hlnum{4}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{figure/r_basics-unnamed-chunk-10-1} 

}


\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{No influential values or outliers}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex}\hlopt{*}\hlstd{Pclass,} \hlkwc{family} \hlstd{= binomial,}
    \hlkwc{data} \hlstd{= titanic)} \hlopt{%>%} \hlkwd{plot}\hlstd{(.,} \hlkwc{which} \hlstd{=} \hlnum{5}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{figure/r_basics-unnamed-chunk-11-1} 

}


\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{No influential values or outliers}
Some solutions for influential values or outliers:
\begin{itemize}
\item remove them
\item keep them but mention them in the result
\item robust logistic regression (\src{robust::glmrob})
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{No influential values or outliers}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{glmRob}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex} \hlopt{+} \hlstd{Pclass,} \hlkwc{family} \hlstd{=} \hlkwd{binomial}\hlstd{(),}
       \hlkwc{data} \hlstd{= titanic,} \hlkwc{method} \hlstd{=} \hlstr{"cubif"}\hlstd{)} \hlopt{%>%}
       \hlkwd{summary}\hlstd{()} \hlopt{%>%} \hlstd{.}\hlopt{$}\hlstd{coefficients}
\end{alltt}
\begin{verbatim}
               Estimate  Std. Error    z value     Pr(>|z|)
(Intercept)  4.76878301 0.452599082  10.536440 5.867834e-26
Age         -0.03433009 0.007412619  -4.631304 3.633694e-06
Sexmale     -2.60924207 0.187095480 -13.946045 3.325391e-44
Pclass      -1.17625580 0.119412375  -9.850368 6.829363e-23
\end{verbatim}
\begin{alltt}
\hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex} \hlopt{+} \hlstd{Pclass,} \hlkwc{family} \hlstd{= binomial,} \hlkwc{data} \hlstd{= titanic)} \hlopt{%>%}
  \hlkwd{summary}\hlstd{()} \hlopt{%>%} \hlstd{.}\hlopt{$}\hlstd{coefficients}
\end{alltt}
\begin{verbatim}
               Estimate  Std. Error    z value     Pr(>|z|)
(Intercept)  4.73195642 0.449819406  10.519680 7.011095e-26
Age         -0.03342722 0.007347635  -4.549385 5.380296e-06
Sexmale     -2.61196394 0.186608818 -13.997002 1.625866e-44
Pclass      -1.16846287 0.118940571  -9.823922 8.881931e-23
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}

%------------------------------------------------------------------------------%

\subsection{Residuals}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Raw residual}
The most basic residual is the \emph{raw residual}, which is the difference between the observed value and the predicted probability:
\begin{align*}
    e_i = y_i - \hat{p_i}
  \end{align*}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex}\hlopt{*}\hlstd{Pclass,} \hlkwc{family} \hlstd{= binomial,}
            \hlkwc{data} \hlstd{= titanic)} \hlopt{%>%} \hlkwd{residuals}\hlstd{(.,} \hlkwc{type} \hlstd{=} \hlstr{"response"}\hlstd{)} \hlopt{%>%}
  \hlkwd{head}\hlstd{()}
\end{alltt}
\begin{verbatim}
          1           2           3           4           5 
-0.14000292  0.01770869  0.50655145  0.01598547 -0.09393664 
          6 
-0.11080960 
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Pearson residual}
The Pearson residual is a scaled version of the raw residual. 
\begin{align*}
    r_i = \frac{e_i}{\sqrt{\hat{p_i}(1-\hat{p_i})}}
  \end{align*}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex}\hlopt{*}\hlstd{Pclass,} \hlkwc{family} \hlstd{= binomial,}
            \hlkwc{data} \hlstd{= titanic)} \hlopt{%>%} \hlkwd{residuals}\hlstd{(.,} \hlkwc{type} \hlstd{=} \hlstr{"pearson"}\hlstd{)} \hlopt{%>%}
  \hlkwd{head}\hlstd{()}
\end{alltt}
\begin{verbatim}
         1          2          3          4          5 
-0.4034782  0.1342682  1.0131899  0.1274565 -0.3219869 
         6 
-0.3530135 
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Pearson residual}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex}\hlopt{*}\hlstd{Pclass,} \hlkwc{family} \hlstd{= binomial,}
            \hlkwc{data} \hlstd{= titanic)} \hlopt{%>%} \hlkwd{plot}\hlstd{(.,} \hlkwc{which} \hlstd{=} \hlnum{1}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{figure/r_basics-unnamed-chunk-15-1} 

}


\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Deviance residual}
Deviance residuals can be approximated with a standard normal distribution if the model fits well.
\begin{align*}
    d_i = sign(e_i)[-2(y_iln\hat{p_i} + (1-y_i)ln(1-\hat{p_i}))]^{1/2}
  \end{align*}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex}\hlopt{*}\hlstd{Pclass,} \hlkwc{family} \hlstd{= binomial,}
    \hlkwc{data} \hlstd{= titanic)} \hlopt{%T>%}
  \hlstd{\{}\hlkwd{residuals}\hlstd{(.,} \hlkwc{type} \hlstd{=} \hlstr{"deviance"}\hlstd{)} \hlopt{%>%}
      \hlkwd{head}\hlstd{(.,} \hlkwc{n} \hlstd{=} \hlnum{6}\hlstd{)} \hlopt{%>%}
      \hlkwd{print}\hlstd{()\}} \hlopt{%>%}
  \hlkwd{summary}\hlstd{()} \hlopt{%>%} \hlstd{.}\hlopt{$}\hlstd{deviance.resid} \hlopt{%>%}
  \hlkwd{head}\hlstd{(.,} \hlkwc{n} \hlstd{=} \hlnum{6}\hlstd{)}
\end{alltt}
\begin{verbatim}
         1          2          3          4          5 
-0.5492291  0.1890363  1.1885594  0.1795250 -0.4441757 
         6 
-0.4846522 
         1          2          3          4          5 
-0.5492291  0.1890363  1.1885594  0.1795250 -0.4441757 
         6 
-0.4846522 
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Residual deviance}
The residual deviance is the sum of squared deviance residuals.  
\begin{align*}
    D = \sum_{i = 1}^{N}d_{i}^2
  \end{align*}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex}\hlopt{*}\hlstd{Pclass,} \hlkwc{family} \hlstd{= binomial,}
            \hlkwc{data} \hlstd{= titanic)} \hlopt{%>%}
  \hlkwd{summary}\hlstd{()} \hlopt{%>%} \hlstd{.}\hlopt{$}\hlstd{deviance}
\end{alltt}
\begin{verbatim}
[1] 781.1453
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Residual deviance}
The residual deviance is used to measure how well the model fits the data. It is similar to the sum of squared errors in linear regression. 
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{result} \hlkwb{<-} \hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex}\hlopt{*}\hlstd{Pclass,} \hlkwc{family} \hlstd{= binomial,}
            \hlkwc{data} \hlstd{= titanic)} \hlopt{%>%} \hlkwd{summary}\hlstd{()}
\hlnum{1} \hlopt{-} \hlkwd{pchisq}\hlstd{(result}\hlopt{$}\hlstd{null.deviance} \hlopt{-} \hlstd{result} \hlopt{$}\hlstd{deviance,}
           \hlstd{result}\hlopt{$}\hlstd{df.null} \hlopt{-} \hlstd{result}\hlopt{$}\hlstd{df.residual)}
\end{alltt}
\begin{verbatim}
[1] 0
\end{verbatim}
\end{kframe}
\end{knitrout}
\begin{align*}
\texttt{null}:& \texttt{$\beta_{1} = \beta_{2} = \dots = \beta_{p} =$ 0} \\
\texttt{alt}:& \texttt{at least one of the $\beta_{i}$ is different from 0}
\end{align*}
\vc
Generally, if the value is less than 0.05, the logistic regression is overall significant.  
\end{frame}

%------------------------------------------------------------------------------%

\subsection{Model selection}

%------------------------------------------------------------------------------%
\begin{frame}[fragile]{Model selection}
Because including irrelevant variables leads to a needlessly complex model, we need to make the model selection among a large set of candidate models.  
\va
I will introduce two techniques:
\begin{itemize}
\item forward selection
\item backward selection
\end{itemize}

\end{frame}
%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Forward selection}
\begin{itemize}
\item Start with the null model (a model contains no variables)
\item Include the most important variable in the model from the remaining predictors 
\item repeat step 2 until a stopping rule is reached
\end{itemize}
\va
The selection criterion defines the most important variable. For example,
If the criterion is AIC, the added variable should make the model has the lowest AIC.

\end{frame}
%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Backward selection}
\begin{itemize}
\item Start with the full model (a model contains all possible variables)
\item Remove the least important variable from the model
\item repeat step 2 until a stopping rule is reached
\end{itemize}
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Model selection}
Here, I show the backward selection by minimizing AIC. The stopping rule is that removal of the least important variable results in the increase of AIC.
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{.}\hlopt{^}\hlnum{2}\hlstd{,} \hlkwc{family} \hlstd{= binomial,} \hlkwc{data} \hlstd{= titanic)} \hlopt{%>%}
  \hlkwd{step}\hlstd{(}\hlkwc{trace} \hlstd{=} \hlnum{0}\hlstd{)}
\end{alltt}
\begin{verbatim}

Call:  glm(formula = Survived ~ Pclass + Age + Sex + Pclass:Sex + Age:Sex, 
    family = binomial, data = titanic)

Coefficients:
   (Intercept)          Pclass             Age  
       6.73391        -2.09689        -0.01681  
       Sexmale  Pclass:Sexmale     Age:Sexmale  
      -4.86677         1.19508        -0.02689  

Degrees of Freedom: 890 Total (i.e. Null);  885 Residual
Null Deviance:	    1187 
Residual Deviance: 778.4 	AIC: 790.4
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}

%------------------------------------------------------------------------------%

\subsection{Confusion matrix}

%------------------------------------------------------------------------------%
\begin{frame}[fragile]{Confusion matrix}
One of the most direct ways to evaluate classification performance is the \emph{Confusion matrix}.
% latex table generated in R 4.2.1 by xtable 1.8-4 package
% Thu Jan  5 23:45:38 2023
\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
  & \multicolumn{2}{c}{True} \\
 Predicted & Not survived & Survived \\
 \hline
Not survived & 507 & 144 \\ 
  Survived & 42 & 198 \\ 
   \hline
\end{tabular}
\caption{Confusion Matrix of passengers' survival on the
Titanic} 
\end{table}


\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Confusion matrix}
In the titanic example,
\begin{itemize}
\item \textbf{TP}: correctly predict people that survived
\item \textbf{TN}: correctly predict people that did not survive
\item \textbf{FP}: predict people survived, when they did not 
\item \textbf{FN}: predict people did not survive, but they did
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Confusion matrix}
\begin{itemize}
\item \textbf{true positive(TP)}: A test result that correctly indicates the presence of a condition.
\item \textbf{true negative(TN)}: A test result that correctly indicates the absence of a condition.
\item \textbf{false positive(FP)}: A test result which wrongly indicates that a particular condition is present. 
\item \textbf{false negative(FN)}: A test result which wrongly indicates that a particular condition is absent. 
\end{itemize}
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Confusion matrix}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/r_basics-unnamed-chunk-21-1} 

}


\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Confusion matrix}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{Confusion.matrix}\hlkwb{<-} \hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex}\hlopt{*}\hlstd{Pclass,} \hlkwc{family} \hlstd{= binomial,}
    \hlkwc{data} \hlstd{= titanic)} \hlopt{%>%} \hlkwd{predict}\hlstd{(.,} \hlkwc{type} \hlstd{=} \hlstr{"response"}\hlstd{)} \hlopt{%>%}
  \hlstd{\{}\hlkwd{ifelse}\hlstd{(.} \hlopt{>} \hlnum{0.5}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{0}\hlstd{)\}} \hlopt{%>%} \hlkwd{as.factor}\hlstd{()} \hlopt{%>%}
  \hlkwd{confusionMatrix}\hlstd{(.,} \hlkwc{reference} \hlstd{=} \hlkwd{as.factor}\hlstd{(titanic}\hlopt{$}\hlstd{Survived),}
                  \hlkwc{positive} \hlstd{=} \hlstr{"1"}\hlstd{)}
\hlstd{Confusion.matrix}\hlopt{$}\hlstd{table}
\end{alltt}
\begin{verbatim}
          Reference
Prediction   0   1
         0 507 144
         1  42 198
\end{verbatim}
\begin{alltt}
\hlstd{Confusion.matrix}\hlopt{$}\hlstd{overall}
\end{alltt}
\begin{verbatim}
      Accuracy          Kappa  AccuracyLower  AccuracyUpper 
  7.912458e-01   5.323785e-01   7.630562e-01   8.174942e-01 
  AccuracyNull AccuracyPValue  McnemarPValue 
  6.161616e-01   2.391263e-29   1.304808e-13 
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Confusion matrix}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{Confusion.matrix}\hlopt{$}\hlstd{byClass}
\end{alltt}
\begin{verbatim}
         Sensitivity          Specificity 
           0.5789474            0.9234973 
      Pos Pred Value       Neg Pred Value 
           0.8250000            0.7788018 
           Precision               Recall 
           0.8250000            0.5789474 
                  F1           Prevalence 
           0.6804124            0.3838384 
      Detection Rate Detection Prevalence 
           0.2222222            0.2693603 
   Balanced Accuracy 
           0.7512223 
\end{verbatim}
\end{kframe}
\end{knitrout}


\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Summary of the confusion matrix}
\emph{Accuracy}:
\begin{itemize}
\item accuracy = (TP + TN) / (P + N)
\item In titanic example, accuracy = 0.79, meaning that 79\% are correctly classified.
\end{itemize}
\va
\emph{Error rate}:
\begin{itemize}
\item error rate = (FP + FN) / (P + N) = 1 - accuracy
\item In titanic example, error rate = 0.21, meaning that 21\% are not correctly classified.
\end{itemize}
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Summary of the confusion matrix}
\emph{Sensitivity}:
\begin{itemize}
\item sensitivity = TP / (TP + FN)
\item In titanic example, sensitivity = 0.58, meaning that if the passenger did survive, there is a 58\% chance the model will detect this.
\end{itemize}
\va
\emph{Specificity}:
\begin{itemize}
\item specificity = TN / (TN + FP)
\item In titanic example, specificity = 0.92, meaning that if the passenger did not survive, there is a 92\% chance the model will detect this. 
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Summary of the confusion matrix}
\emph{False positive rate}:
\begin{itemize}
\item false positive rate (FPR) = FP / (TN + FP) = 1 - specificity
\item In titanic example, FPR = 0.08, meaning that if a passenger did not survive, there is an 8\% chance that the model predicts this passenger as surviving. 
\end{itemize}
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Summary of the confusion matrix}
\emph{Positive predictive value}:
\begin{itemize}
\item positive predictive value (PPV) = TP / (TP + FP)
\item In titanic example, PPV = 0.83, meaning that if the passenger is predicted as surviving, there is an 83\% chance that this passenger indeed survived. 
\end{itemize}
\va
\emph{Negative predictive value}:
\begin{itemize}
\item negative predictive value (NPV) = TN / (TN + FN)
\item In titanic example, NPV = 0.78, meaning that if a passenger is predicted as not surviving, there is a 78\% chance that this passenger indeed does not survive.  
\end{itemize}
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{ROC curve}
A receiver operating characteristic curve (ROC curve) is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve is created by plotting sensitivity against FPR (1 - specificity) at various threshold values. 
\va

ROC curve is mainly used for:
\begin{itemize}
\item evaluating the classification performance
\item selecting discrimination threshold
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{ROC curve}
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex}\hlopt{*}\hlstd{Pclass,} \hlkwc{family} \hlstd{= binomial,}
    \hlkwc{data} \hlstd{= titanic)} \hlopt{%>%} \hlkwd{predict}\hlstd{(.,} \hlkwc{type} \hlstd{=} \hlstr{"response"}\hlstd{)} \hlopt{%>%}
  \hlstd{ROCR}\hlopt{::}\hlkwd{prediction}\hlstd{(.,} \hlkwd{as.factor}\hlstd{(titanic}\hlopt{$}\hlstd{Survived))} \hlopt{%>%}
  \hlstd{ROCR}\hlopt{::}\hlkwd{performance}\hlstd{(.,} \hlstr{"tpr"}\hlstd{,} \hlstr{"fpr"}\hlstd{)} \hlopt{%>%}
  \hlkwd{plot}\hlstd{()}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{figure/r_basics-unnamed-chunk-24-1} 

}


\end{knitrout}
\end{frame}

%------------------------------------------------------------------------------%
\begin{frame}[fragile]{Sensitivity versus specificity trade-off}
Since sensitivity has a positive correlation with the false positive rate, there is a trade-off between sensitivity and specificity. 

\end{frame}
%------------------------------------------------------------------------------%
\begin{frame}[fragile]{ROC curve}
The Area Under the ROC Curve (AUC) summarizes the performance of the classification.
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex}\hlopt{*}\hlstd{Pclass,} \hlkwc{family} \hlstd{= binomial,}
    \hlkwc{data} \hlstd{= titanic)} \hlopt{%>%} \hlkwd{predict}\hlstd{(.,} \hlkwc{type} \hlstd{=} \hlstr{"response"}\hlstd{)} \hlopt{%>%}
  \hlstd{pROC}\hlopt{::}\hlkwd{roc}\hlstd{(}\hlkwd{as.factor}\hlstd{(titanic}\hlopt{$}\hlstd{Survived), .)} \hlopt{%>%}
  \hlkwd{auc}\hlstd{()}
\end{alltt}
\begin{verbatim}
Area under the curve: 0.8497
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{itemize}
\item AUC value from 0.7-0.8: acceptable
\item AUC value from 0.8-0.9: excellent
\item AUC value over 0.9: outstanding \citep{mandrekar2010receiver}
\end{itemize}
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Threshold selection}
Sometimes, we do not want to use 0.5 as the threshold. 
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.878, 0.918, 0.933}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{glm}\hlstd{(Survived} \hlopt{~} \hlstd{Age} \hlopt{+} \hlstd{Sex}\hlopt{*}\hlstd{Pclass,} \hlkwc{family} \hlstd{= binomial,}
    \hlkwc{data} \hlstd{= titanic)} \hlopt{%>%} \hlkwd{predict}\hlstd{(.,} \hlkwc{type} \hlstd{=} \hlstr{"response"}\hlstd{)} \hlopt{%>%}
  \hlkwd{mutate}\hlstd{(titanic,} \hlkwc{pred} \hlstd{= .)} \hlopt{%>%}
  \hlstd{OptimalCutpoints}\hlopt{::}\hlkwd{optimal.cutpoints}\hlstd{(}\hlkwc{X} \hlstd{= pred}  \hlopt{~} \hlstd{Survived,}
                                      \hlkwc{tag.healthy} \hlstd{=} \hlnum{0}\hlstd{,}
                        \hlkwc{control} \hlstd{=} \hlkwd{control.cutpoints}\hlstd{(),}
                        \hlkwc{methods} \hlstd{=} \hlstr{"ROC01"}\hlstd{,}
                        \hlkwc{data} \hlstd{= .,} \hlkwc{ci.fit} \hlstd{=} \hlnum{FALSE}\hlstd{,}
                        \hlkwc{conf.level} \hlstd{=} \hlnum{0.95}\hlstd{,}
                        \hlkwc{trace} \hlstd{=} \hlnum{FALSE}\hlstd{)} \hlopt{%>%}
      \hlstd{.}\hlopt{$}\hlstd{ROC01} \hlopt{%>%} \hlstd{.}\hlopt{$}\hlstd{Global} \hlopt{%>%} \hlstd{.}\hlopt{$}\hlstd{optimal.cutoff} \hlopt{%>%} \hlstd{.}\hlopt{$}\hlstd{cutoff}
\end{alltt}
\begin{verbatim}
[1] 0.3530982
\end{verbatim}
\end{kframe}
\end{knitrout}

This threshold minimizes the distance between the selected point on the ROC plot and point (0, 1). 
\end{frame}

%------------------------------------------------------------------------------


\begin{frame}[fragile]{Weight sensitivity or specificity?}
Selecting a point with the smallest distance to the point (0, 1) is to maximize $sensitivity ^2 + specificity ^ 2$. This optimized function has equal weights to sensitivity and specificity. However, in some scenarios, we care more about sensitivity or specificity.  
\end{frame}


%------------------------------------------------------------------------------
\begin{frame}[fragile]{Weight sensitivity or specificity?}
\begin{itemize}
\item \emph{When sensitivity is more important}
\begin{itemize}
\item Predict whether a patient has a specific disease.
\end{itemize}
\vb
\item \emph{When specificity is more important}
\begin{itemize}
\item Predict whether a person has committed a crime.
\end{itemize}
\end{itemize}
\end{frame}





%------------------------------------------------------------------------------

\begin{filecontents}{ftds_refs_w7.bib}
@Article{mandrekar2010receiver,
  author    = {Mandrekar, Jayawant N},
  journal   = {Journal of Thoracic Oncology},
  title     = {Receiver operating characteristic curve in diagnostic test assessment},
  year      = {2010},
  number    = {9},
  pages     = {1315--1316},
  volume    = {5},
  publisher = {Elsevier},
}

@Book{agresti2018introduction,
  author    = {Agresti, Alan},
  publisher = {John Wiley \& Sons},
  title     = {An introduction to categorical data analysis},
  year      = {2018},
}

@Article{peduzzi1996simulation,
  author    = {Peduzzi, Peter and Concato, John and Kemper, Elizabeth and Holford, Theodore R and Feinstein, Alvan R},
  journal   = {Journal of clinical epidemiology},
  title     = {A simulation study of the number of events per variable in logistic regression analysis},
  year      = {1996},
  number    = {12},
  pages     = {1373--1379},
  volume    = {49},
  publisher = {Elsevier},
}

@Article{bujang2018review,
  author    = {Bujang, Mohamad Adam and Omar, Evi Diana and Baharum, Nur Akmal},
  journal   = {The Malaysian journal of medical sciences: MJMS},
  title     = {A review on sample size determination for Cronbachâ€™s alpha test: a simple guide for researchers},
  year      = {2018},
  number    = {6},
  pages     = {85},
  volume    = {25},
  publisher = {School of Medical Sciences, Universiti Sains Malaysia},
}
\end{filecontents}

\let\origharvardyearright\harvardyearright
\makeatletter
\renewcommand\harvardyearright{\origharvardyearright\@ifnextchar,\@gobble\empty}
\makeatother

\begin{frame}{References}
\bibliographystyle{apacite}
\bibliography{ftds_refs_w7}
\end{frame}

\end{document}
